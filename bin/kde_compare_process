#!/usr/bin/env python

__doc__ = "generate statistics of processes from CSV files which refer to EOS tables (other csv files) as a corner plot at a few reference points. Uses KDEs to make the plots pretty"
__usage__ = "kde_compare_process [--options] label1,samples1.csv label2,samples2.csv reference_column column [column column ...]"
__author__ = "reed.essick@ligo.org"

#-------------------------------------------------

import os
import numpy as np

from optparse import OptionParser

### non-standard libraries
from universality import utils
from universality import stats

#-------------------------------------------------

parser = OptionParser(usage=__usage__, description=__doc__)

parser.add_option('-v', '--verbose', default=False, action='store_true')
parser.add_option('-V', '--Verbose', default=False, action='store_true')

parser.add_option('', '--logcolumn', default=[], type='string', action='append',
    help='convert the values read in for this column to natural log. \ 
Can be repeated to specify multiple columns. \
DEFAULT=[]')

parser.add_option('', '--weight1-column', default=None, type='string',
    help='if provided, thie numerical values from this column will be used as weights in the KDE')
parser.add_option('', '--weight1-column-is-log', default=False, action='store_true',
    help='if supplied, interpret the values in weight_column as log(weight), meaning we exponentiate them before using them in the KDE')

parser.add_option('', '--weight2-column', default=None, type='string',
    help='if provided, thie numerical values from this column will be used as weights in the KDE')
parser.add_option('', '--weight2-column-is-log', default=False, action='store_true',
    help='if supplied, interpret the values in weight_column as log(weight), meaning we exponentiate them before using them in the KDE')

parser.add_option('-r', '--reference-value', default=[], type='float', action='append',
    help='the reference values at which we extract values from the EOS parameters. \
DEFAULT=[]')

parser.add_option('', '--eos1-column', default='eos', type='string')
parser.add_option('', '--eos1-dir', default='.', type='string')
parser.add_option('', '--eos1-basename', default='draw_foo-%d.csv', type='string',
    help='a string into which the EOS number can be substituted when building filenames. \
DEFAULT=draw_foo-%d.csv')

parser.add_option('', '--eos2-column', default='eos', type='string')
parser.add_option('', '--eos2-dir', default='.', type='string')
parser.add_option('', '--eos2-basename', default='draw_foo-%d.csv', type='string',
    help='a string into which the EOS number can be substituted when building filenames. \
DEFAULT=draw_foo-%d.csv')

parser.add_option('--num-points', default=plot.DEFAULT_NUM_POINTS, type='int',
    help='DEFAULT=%d'%plot.DEFAULT_NUM_POINTS)
parser.add_option('', '--bandwidth', nargs=3, default=[], type='string', action='append',
    help='the bandwidths used for each column specified at each reference value. e.g.: "col ${ref_val} ${bandwidth}". \
We assume diagonal covariance matricies in the Gaussian kernel. \
If you do not specify a bandwidth for a column, the default value (%.3f) will be used.'%utils.DEFAULT_BANDWIDTH)
parser.add_option('', '--reflect', default=False, action='store_true',
    help='reflect the points about their boundaries within the KDE')
parser.add_option('', '--purne', default=False, action='store_true',
    help='throw away samples outside of the specified range')

### options about what to compute
parser.add_option('--dkl', default=False, action='store_true')
parser.add_option('--sym-dkl', default=False, action='store_true')
parser.add_option('--dlogL', default=False, action='store_true')
parser.add_option('--dtheta', default=False, action='store_true')

opts, args = parser.parse_args()
assert len(opts.reference_value), 'please supply at least one --reference-value\n%s'%__usage__
opts.reference_value.sort()

assert len(args)>3, 'please supply at least 4 input argument\n%s'%__usage__
inpath1, inpath2, reference = args[:3]
label1, inpath1 = inpath1.split(',')
label2, inpath2 = inpath2.split(',')
columns = args[3:]
    
opts.verbose |= opts.Verbose 

opts.dlogL = [[float(_) for _ in v.split(',')] for v in opts.dlogL]
opts.dtheta = [[float(_) for _ in v.split(',')] for v in opts.dtheta]

bandwidthdict = dict((col, {}) for col in columns)
for col, ref, val in opts.bandwidth:
    bandwidthdict[col][float(ref)] = float(val)
bandwidths = []
for column in columns:
    bandwidths += [bandwidthdict[column].get(value, utils.DEFAULT_BANDWIDTH) for value in opts.reference_value]
variances = np.array(bandwidths)**2

#-------------------------------------------------

if opts.verbose:
    print('reading samples for %s from: %s'%(label1, inpath1))
data1, _ = utils.load(inpath1, [opts.eos1_column])
data1 = data1[:,0]
N1 = len(data1)

if opts.weight1_column!=None:
    if opts.verbose:
        print('reading in non-trivial weights for %s from: %s'%(label1, inpath1))
    weights1, _ = utils.load(inpath1, [opts.weight1_column])
    weights1 = weights1.reshape((len(weights1)))

    if opts.weight1_column_is_log:
        weights1 = np.exp(weights1-np.max(weights1))
    weights1 /= np.sum(weights1)

else:
    weights1 = np.ones(N1, dtype='float')/N1

if opts.verbose:
    print('reading samples for %s from: %s'%(label2, inpath2))
data2, _ = utils.load(inpath2, [opts.eos2_column])
data2 = data2[:,0]
N2 = len(data2)

if opts.weight2_column!=None:
    if opts.verbose:
        print('reading in non-trivial weights for %s from: %s'%(label2, inpath2))
    weights2, _ = utils.load(inpath2, [opts.weight2_column])
    weights2 = weights2.reshape((len(weights2)))

    if opts.weight2_column_is_log:
        weights2 = np.exp(weights2-np.max(weights2))
    weights2 /= np.sum(weights2)

else:
    weights2 = np.ones(N2, dtype='float')/N2

#------------------------

if opts.verbose:
    print('extacting data at reference values for '+label1)
Ncol = len(columns)
Nref = len(opts.reference_value)

loadcolumns = [reference]+columns
refind = loadcolumns.index(reference)

ans1 = np.empty((N1, Nref*Ncol), dtype='float')
for i, eos in enumerate(data1):
    path = os.path.join(opts.eos1_dir, opts.eos1_basename%eos)
    if opts.Verbose:
        print('    '+path)
    d, c = utils.load(path, loadcolumns)

    for j, column in enumerate(c[1:]):
        ans1[i,j*Nref:(j+1)*Nref] = np.interp(opts.reference_value, d[:,refind], d[:,loadcolumns.index(column)])

ans2 = np.empty((N2, Nref*Ncol), dtype='float')
for i, eos in enumerate(data2):
    path = os.path.join(opts.eos2_dir, opts.eos2_basename%eos)
    if opts.Verbose:
        print('    '+path)
    d, c = utils.load(path, loadcolumns)

    for j, column in enumerate(c[1:]):
        ans2[i,j*Nref:(j+1)*Nref] = np.interp(opts.reference_value, d[:,refind], d[:,loadcolumns.index(column)])

#------------------------

if verbose:
    print('computing kde')
ranges = []
for i in xrange(Ncol):
    m1, M2 = utils.data2range(data1[:,i])
    m2, M2 = utils.data2range(data2[:,i])
    ranges.append((min(m1,m2), max(M1,M2)))

vects = [np.linspace(m, M, opts.num_points) for m, M in ranges]
flatgrid = utils.vects2flatgrid(*vects)

if opts.prune:
    data1, weights1 = utils.prune(data1, ranges, weights=weights1)
if opts.reflect:
    data1, weights1 = utils.reflect(data1, ranges, weights=weights1)
logkde1 = utils.logkde(
    flatgrid,
    data1,
    variances,
    weights=weights1,
)

if opts.prune:
    data2, weights2 = utils.prune(data2, ranges, weights=weights2)
if opts.reflect:
    data2, weights2 = utils.reflect(data2, ranges, weights=weights2)
logkde2 = utils.logkde(
    flatgrid,
    data2,
    variances,
    weights=weights2,
)

if verbose:
    print('computing statistics')

if opts.dkl:
    print('Dkl(1||2) = %.6e'%stats.kldiv(vects, logkde1, logkde2))
    print('Dkl(2||1) = %.6e'%stats.kldiv(vects, logkde2, logkde1))

if opts.sym_dkl:
    print('sym Dkl(1,2) = %.6e'%stats.sym_kldiv(vects, logkde1, logkde2))

if opts.dlogL:
    print('dlogL1(argmax2) = %.6e'%utils.logkde([stats.argmax(vects, logkde2)], data1, variances, weights=weights1) - np.max(logkde1))
    print('dlogL2(argmax1) = %.6e'%utils.logkde([stats.argmax(vects, logkde1)], data2, variances, weights=weights2) - np.max(logkde2))

if opts.dtheta:
    dtheta = stats.argmax(vects, logkde1) - stats.argmax(vects, logkde2)
    print('dtheta = %s'%dtheta)
    print('|dtheta| = %.6e'%np.sum(dtheta**2)**0.5)
