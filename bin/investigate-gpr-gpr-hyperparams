#!/usr/bin/env python

__description__ = "read in existing processes from disk and generate another process over them."
__author__ = "reed.essick@ligo.org"

#---------------------------------------------------------------------------------------------------

import os

import numpy as np
import h5py

from argparse import ArgumentParser

### non-standard
from universality import utils
from universality import gaussianprocess as gp
from universality import hyperparams as hp
from universality import plot
from universality import stats

#-------------------------------------------------

DEFAULT_MIN = 1e30 ### g/cm^3
DEFAULT_MAX = 1e38

DEFAULT_STITCH_MEAN = 6.0 ### chosen by eye...
DEFAULT_STITCH_PRESSURE = 1e10*utils.c2 ### dyn/cm^2
DEFAULT_STITCH_INDEX = 5

#-------------------------------------------------

parser = ArgumentParser(description=__doc__)

### required arguments
parser.add_argument_group('required arguments')
parser.add_argument('hdf5paths', nargs='+', type=str)

### verbosity options
vgroup = parser.add_argument_group('verbosity arguments')
vgroup.add_argument('-v', '--verbose', default=False, action='store_true')
vgroup.add_argument('-V', '--Verbose', default=False, action='store_true')
vgroup.add_argument('--VVerbose', default=False, action='store_true')

### options for stitching
sgroup = parser.add_argument_group('stitching options')
sgroup.add_argument('--stitch', default=False, action='store_true')

sgroup.add_argument('--stitch-pressure-bounds', nargs=2, default=None, type=float,
    help='limit the range of stitching points. DEFAULT is to use --pressure-bounds')
sgroup.add_argument('--stitch-num-points', default=None, type=int,
    help='the number of points to use when constructing the stitching conditions. DEFAULT is to use --num-points')

sgroup.add_argument('--stitch-mean', default=DEFAULT_STITCH_MEAN, type=float,
    help='the mean value for stitching points. \
DEFAULT=%.3f'%DEFAULT_STITCH_MEAN)
sgroup.add_argument('--stitch-pressure', default=DEFAULT_STITCH_PRESSURE, type=float,
    help='the abscissa at which we place a roll-off to enforce stitching. \
We construct a white-noise kernel ~ (p/stitch_pressure)**stitch_index. \
DEFAULT=%.3f'%DEFAULT_STITCH_PRESSURE)
sgroup.add_argument('--stitch-index', default=DEFAULT_STITCH_INDEX, type=float,
    help='the power used to roll-off the white-noise kernel and enforce stitching. \
We construct a white-noise kernel ~ (p/stitch_pressure)**stitch_index. \
DEFAULT=%.3f'%DEFAULT_STITCH_INDEX)

### bounds on hyperparams
parser.add_argument('--poly-degree', default=1, type=int)

parser.add_argument('--min-sigma', default=hp.DEFAULT_MIN_SIGMA, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MIN_SIGMA)
parser.add_argument('--max-sigma', default=hp.DEFAULT_MAX_SIGMA, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MAX_SIGMA)

parser.add_argument('--min-l', default=hp.DEFAULT_MIN_L, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MIN_L)
parser.add_argument('--max-l', default=hp.DEFAULT_MAX_L, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MAX_L)

parser.add_argument('--min-sigma_obs', default=hp.DEFAULT_MIN_SIGMA, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MIN_SIGMA)
parser.add_argument('--max-sigma_obs', default=hp.DEFAULT_MAX_SIGMA, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MAX_SIGMA)

parser.add_argument('--min-m', default=hp.DEFAULT_MIN_M, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MIN_M)
parser.add_argument('--max-m', default=hp.DEFAULT_MAX_M, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MAX_M)

parser.add_argument('--num-mc', default=gp.DEFAULT_NUM, type=int,
    help='the number of monte carlo draws through the hyperparameter priors. \
DEFAULT=%d'%gp.DEFAULT_NUM)

parser.add_argument('--num-grid', default=gp.DEFAULT_NUM, type=int,
    help='the number of grid points in each dimension. \
DEFAULT=%d'%gp.DEFAULT_NUM)

parser.add_argument('--num-mcmc', default=hp.DEFAULT_NUM_MCMC, type=int,
    help='the number of samples to draw in mcmc. \
DEFAULT=%d'%hp.DEFAULT_NUM_MCMC)

parser.add_argument('--num-walkers', default=hp.DEFAULT_NUM_WALKERS, type=int,
    help='DEFAULT=%d'%hp.DEFAULT_NUM_WALKERS)

#parser.add_argument('--maxL-method', default=hp.DEFAULT_METHOD, type=str,
#    help='DEFAULT=%s'%hp.DEFAULT_METHOD)
#
#parser.add_argument('--maxL-tol', default=hp.DEFAULT_TOL, type=float,
#    help='DEFAULT=%s'%hp.DEFAULT_TOL)

parser.add_argument('--l-prior', default=hp.DEFAULT_L_PRIOR, type=str)
parser.add_argument('--sigma-prior', default=hp.DEFAULT_SIGMA_PRIOR, type=str)
parser.add_argument('--sigma_obs-prior', default=hp.DEFAULT_SIGMA_PRIOR, type=str)
parser.add_argument('--m-prior', default=hp.DEFAULT_M_PRIOR, type=str)
parser.add_argument('--diagonal-model-covariance', default=False, action='store_true')

parser.add_argument('--temperature', default=hp.DEFAULT_TEMPERATURE, type=float,
    help='the temperature used to flatten the likelihood a la parallel tempering. \
DEFAULT=%.3f'%hp.DEFAULT_TEMPERATURE)

parser.add_argument('--mc', default=False, action='store_true')
parser.add_argument('--grid', default=False, action='store_true')
parser.add_argument('--mcmc', default=False, action='store_true')
#parser.add_argument('--maxL', default=False, action='store_true')

parser.add_argument('--strip-mcmc', default=0, type=int,
    help='the number of burn in samples to reject from the mcmc sampler')

parser.add_argument('--num-proc', default=utils.DEFAULT_NUM_PROC, type=int)
parser.add_argument('--slow-cvlogLike', default=False, action='store_true',
    help='use a computationally slower method to evaluate the cross validation log likelihood, but it does appear to be more numerically stable')

parser.add_argument('-p', '--plot', default=False, action='store_true')

parser.add_argument('--log-sigmas', default=False, action='store_true')
parser.add_argument('--include-logL', default=False, action='store_true')

parser.add_argument('-o', '--output-dir', default='.', type=str)
parser.add_argument('-t', '--tag', default='', type=str)
parser.add_argument('--figtype', default=[], type=str, action='append')
parser.add_argument('--dpi', default=plot.DEFAULT_DPI, type=float)

args = parser.parse_args()

#assert np.any((args.grid, args.mcmc, args.mc, args.maxL)), 'please specify at least one of: --grid, --mcmc, --mcmc, --maxL'
assert np.any((args.mc, args.grid, args.mcmc)), 'please specify at least one of: --mc, --grid, --mcmc'

### finish parsing
Nhdf5 = len(args.hdf5paths)

if args.stitch:
    if args.stitch_pressure_bounds is None:
        args.stitch_pressure_bounds = args.pressure_bounds
    if args.stitch_num_points is None:
        args.stitch_num_points = args.num_points

if not os.path.exists(args.output_dir):
    os.makedirs(args.output_dir)

if args.tag:
    args.tag = "_"+args.tag

if not args.figtype:
    args.figtype = plot.DEFAULT_FIGTYPES

args.Verbose |= args.VVerbose
args.verbose |= args.Verbose

#-------------------------------------------------

models = []
for hdf5path in args.hdf5paths:
    if args.verbose:
        print('reading: '+hdf5path)
    model = gp.hdf5load(hdf5path)
    if args.Verbose:
        if len(model)==1:
            print('    found mixture model with 1 element')
        else:
            print('    found mixture model with %d elements'%len(model))
    models.append(model)

### create combinatorically many possible matchings between all models
inds = zip(*[_.flatten() for _ in np.meshgrid(*[range(len(model)) for model in models])])
Ninds = len(inds)

#-------------------------------------------------

### add stitching stuff
if args.stitch: ### set up stitching conditions
    if args.verbose:
        print('enforcing stitching to f=%.3e with pressure-scale=%.3e dyn/cm^2 and index=%.3f at %d points within  [%.3e, %.3e] dyn/cm^2'%\
            ((args.stitch_mean, args.stitch_pressure, args.stitch_index, args.stitch_num_points)+tuple(args.stitch_pressure_bounds)))
    x_stitch = np.linspace(
        np.log(args.stitch_pressure_bounds[0]),
        np.log(args.stitch_pressure_bounds[1]),
        args.stitch_num_points,
    )
    x_stitch -= 2*np.log(utils.c) ### divide by c^2 to get this into consistent units with gpr-resample, etc
    f_stitch, cov_stitch = gp.cov_phi_phi_stitch(x_stitch, args.stitch_mean, args.stitch_pressure, args.stitch_index)
    stitch = [{'x':x_stitch, 'f':f_stitch, 'cov':cov_stitch}]
else:
    stitch = []

#------------------------

#--- now do some logLikelihood stuff!

if args.plot:
    overlayfig = None

    if args.log_sigmas:
        labels = ['$\log_{10}\sigma$', '$l$', '$\log_{10}\sigma_\mathrm{obs}$', '$\log_{10}m$']
    else:
        labels = ['$\sigma$', '$l$', '$\sigma_\mathrm{obs}$', '$m$']
    if args.include_logL:
        labels.append('$\log\mathcal{L}$')
    labels = np.array(labels)

    include = np.ones(5 if args.include_logL else 4, dtype=bool)
    if args.min_sigma==args.max_sigma:
        include[0] = False
    if args.min_l==args.max_l:
        include[1] = False
    if args.min_sigma_obs==args.max_sigma_obs:
        include[2] = False
    if args.min_m==args.max_m:
        include[3] = False

truths = None

#---

if args.grid:
    if args.verbose:
        print('evaluating on a %s grid'%("x".join('%d'%args.num_grid for _ in xrange(np.sum(include)-args.include_logL))))

    grid = hp.cvlogLike_grid(
        models,
        stitch,
        (args.min_sigma, args.max_sigma),
        (args.min_l, args.max_l),
        (args.min_sigma_obs, args.max_sigma_obs),
        (args.min_m, args.max_m),
        num_sigma=args.num_grid,
        num_l=args.num_grid,
        num_sigma_obs=args.num_grid,
        num_model_multiplier=args.num_grid,
        sigma_prior=args.sigma_prior,
        l_prior=args.l_prior,
        sigma_obs_prior=args.sigma_obs_prior,
        model_multiplier_prior=args.m_prior,
        degree=args.poly_degree,
        num_proc=args.num_proc,
        temperature=args.temperature,
        slow=args.slow_cvlogLike,
        diagonal_model_covariance=args.diagonal_model_covariance,
        verbose=args.VVerbose,
    )

    ### print maximum likelihood parameters
    if args.verbose:
        print('''\
sigma = %.3e
l     = %.3f
sigma_obs = %.3e
model_multiplier = %.3e
logL = %.3e'''%tuple(grid[grid['logLike'].argmax()]))

    ### write the results to disk
    path = "%s/investigate-gpr-gpr-hyperparams-grid%s.csv"%(args.output_dir, args.tag)
    if args.verbose:
        print('writing: '+path)
    tmp = '%d,'%args.poly_degree + ','.join('%.9e' for _ in xrange(5)) # s, l, S, m, logLike
    with open(path, 'w') as file_obj:
        print >> file_obj, 'poly_degree,sigma,l,sigma_obs,multiplier,logLike'
        for tup in zip(grid['sigma'], grid['l'], grid['sigma_obs'], grid['model_multiplier'], grid['logLike']):
            print >> file_obj, tmp%tup

    if args.plot:
        if args.include_logL:
            if args.log_sigmas:
                data = np.transpose([np.log10(grid['sigma']), grid['l'], np.log10(grid['sigma_obs']), np.log10(grid['model_multiplier']), grid['logLike']])
            else:
                data = np.transpose([grid['sigma'], grid['l'], grid['sigma_obs'], grid['model_multiplier'], grid['logLike']])
        else:
            if args.log_sigmas:
                data = np.transpose([np.log10(grid['sigma']), grid['l'], np.log10(grid['sigma_obs']), np.log10(grid['model_multiplier'])])
            else:
                data = np.transpose([grid['sigma'], grid['l'], grid['sigma_obs'], grid['model_multiplier']])

        weights = np.exp(grid['logLike']-np.max(grid['logLike']))
        weights /= np.sum(weights)

        truth = weights>1e-3*np.max(weights)

        if args.verbose:
            print('    plotting')

        fig = plot.corner(
            data[truth][:,include],
            labels=labels[include],
            weights=weights[truth],
            truths=truths[include] if truths is not None else None,
            color='b',
        )
        fig.text(0.75, 0.75, '%.3f effective samples'%stats.neff(weights), ha='center', va='center')
        plot.save('investigate-gpr-gpr-hyperparams-grid%s'%args.tag, fig, directory=args.output_dir, figtypes=args.figtype, dpi=args.dpi, verbose=args.verbose)
        plot.close(fig)

        ### add to overlay plot
        overlayfig = plot.corner(
            data[truth][:,include],
            labels=labels[include],
            weights=weights[truth],
            color='b',
            truths=truths[include] if truths is not None else None,
            fig=overlayfig,
        )

#---

if args.mc:
    if args.verbose:
        print('generating %d Monte-Carlo samples from the hyperprior'%args.num_mc)
    mc = hp.cvlogLike_mc(
        models,
        stitch,
        (args.min_sigma, args.max_sigma),
        (args.min_l, args.max_l),
        (args.min_sigma_obs, args.max_sigma_obs),
        (args.min_m, args.max_m),
        num_samples=args.num_mc,
        sigma_prior=args.sigma_prior,
        l_prior=args.l_prior,
        sigma_obs_prior=args.sigma_obs_prior,
        model_multiplier_prior=args.m_prior,
        degree=args.poly_degree,
        num_proc=args.num_proc,
        temperature=args.temperature,
        slow=args.slow_cvlogLike,
        diagonal_model_covariance=args.diagonal_model_covariance,
        verbose=args.VVerbose,
    )

    ### print maximum likelihood parameters
    if args.verbose:
        print('''\
sigma = %.3e
l     = %.3f
sigma_obs = %.3e
model_multiplier = %.3e
logL = %.3e'''%tuple(mc[mc['logLike'].argmax()]))

    ### write the results to disk
    path = "%s/investigate-gpr-gpr-hyperparams-mc%s.csv"%(args.output_dir, args.tag)
    if args.verbose:
        print('writing: '+path)
    tmp = '%d,'%args.poly_degree + ','.join('%.9e' for _ in xrange(5)) # s, l, S, m, logLike
    with open(path, 'w') as file_obj:
        print >> file_obj, 'poly_degree,sigma,l,sigma_obs,multiplier,logLike'
        for tup in zip(mc['sigma'], mc['l'], mc['sigma_obs'], mc['model_multiplier'], mc['logLike']):
            print >> file_obj, tmp%tup

    if args.plot:
        if args.include_logL:
            if args.log_sigmas:
                data = np.transpose([np.log10(mc['sigma']), mc['l'], np.log10(mc['sigma_obs']), np.log10(mc['model_multiplier']), mc['logLike']])
            else:
                data = np.transpose([mc['sigma'], mc['l'], mc['sigma_obs'], mc['model_multiplier'], mc['logLike']])
        else:
            if args.log_sigmas:
                data = np.transpose([np.log10(mc['sigma']), mc['l'], np.log10(mc['sigma_obs']), np.log10(mc['model_multiplier'])])
            else:
                data = np.transpose([mc['sigma'], mc['l'], mc['sigma_obs'], mc['model_multiplier']])

        weights = np.exp(mc['logLike']-np.max(mc['logLike']))
        weights /= np.sum(weights)

        truth = weights>1e-3*np.max(weights)

        if args.verbose:
            print('    plotting')

        fig = plot.corner(
            data[truth][:,include],
            labels=labels[include],
            weights=weights[truth],
            truths=truths[include] if truths is not None else None,
            color='g',
        )
        fig.text(0.75, 0.75, '%.3f effective samples'%stats.neff(weights), ha='center', va='center')
        plot.save('investigate-gpr-gpr-hyperparams-mc%s'%args.tag, fig, directory=args.output_dir, figtypes=args.figtype, dpi=args.dpi, verbose=args.verbose)
        plot.close(fig)

        ### add to overlay plot
        overlayfig = plot.corner(
            data[truth][:,include],
            labels=labels[include],
            weights=weights[truth],
            color='g',
            truths=truths[include] if truths is not None else None,
            fig=overlayfig,
        )

if args.mcmc:
    if args.verbose:
        print('running MCMC for %d steps with %d walkers'%(args.num_mc, args.num_walkers))
    mcmc = hp.cvlogLike_mcmc(
        models,
        stitch,
        (args.min_sigma, args.max_sigma),
        (args.min_l, args.max_l),
        (args.min_sigma_obs, args.max_sigma_obs),
        (args.min_m, args.max_m),
        num_samples=args.num_mcmc,
        num_walkers=args.num_walkers,
        sigma_prior=args.sigma_prior,
        l_prior=args.l_prior,
        sigma_obs_prior=args.sigma_obs_prior,
        model_multiplier_prior=args.m_prior,
        degree=args.poly_degree,
        temperature=args.temperature,
        slow=args.slow,
        diagonal_model_covariance=args.diagonal_model_covariance,
    )

    ### print maximum likelihood parameters
    if args.verbose:
        print('''\
sigma = %.3e
l     = %.3f
sigma_obs = %.3e
model_multiplier = %.3e
logL = %.3e'''%tuple(mcmc[mcmc['logLike'].argmax()]))

    ### write the results to disk
    path = "%s/investigate-gpr-gpr-hyperparams-mcmc%s.csv"%(args.output_dir, args.tag)
    if args.verbose:
        print('writing: '+path)
    tmp = '%d,'%args.poly_degree + ','.join('%.9e' for _ in xrange(5)) # s, l, S, m, logLike
    with open(path, 'w') as file_obj:
        print >> file_obj, 'poly_degree,sigma,l,sigma_obs,multiplier,logLike'
        for tup in zip(mcmc['sigma'], mcmc['l'], mcmc['sigma_obs'], mcmc['model_multiplier'], mcmc['logLike']):
            print >> file_obj, tmp%tup

    if args.plot:
        if args.include_logL:
            if args.log_sigmas:
                data = np.transpose([np.log10(mcmc['sigma']), mcmc['l'], np.log10(mcmc['sigma_obs']), np.log10(mcmc['model_multiplier']), mcmc['logLike']])
            else:
                data = np.transpose([mcmc['sigma'], mcmc['l'], mcmc['sigma_obs'], mcmc['model_multiplier'], mcmc['logLike']])
        else:
            if args.log_sigmas:
                data = np.transpose([np.log10(mcmc['sigma']), mcmc['l'], np.log10(mcmc['sigma_obs']), np.log10(mcmc['model_multiplier'])])
            else:
                data = np.transpose([mcmc['sigma'], mcmc['l'], mcmc['sigma_obs'], mcmc['model_multiplier']])

        if args.verbose:
            print('    plotting')

        fig = plot.corner(
            data[:,include],
            labels=labels[include],
            truths=truths[include] if truths is not None else None,
            color='r',
        )
        fig.text(0.75, 0.75, '%.3f effective samples'%stats.neff(weights), ha='center', va='center')
        plot.save('investigate-gpr-gpr-hyperparams-mcmc%s'%args.tag, fig, directory=args.output_dir, figtypes=args.figtype, dpi=args.dpi, verbose=args.verbose)
        plot.close(fig)

        ### add to overlay plot
        overlayfig = plot.corner(
            data[:,include],
            labels=labels[include],
            color='r',
            truths=truths[include] if truths is not None else None,
            fig=overlayfig,
        )

#--- wrap up overlay plot

if args.plot and (overlayfig is not None):
    plot.save('investigate-gpr-gpr-hyperparams%s'%args.tag, overlayfig, directory=args.output_dir, figtypes=args.figtype, dpi=args.dpi, verbose=args.verbose)
    plot.close(overlayfig)
