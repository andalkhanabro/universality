#!/usr/bin/env python

__description__ = "read in existing processes from disk and generate another process over them."
__author__ = "reed.essick@ligo.org"

#---------------------------------------------------------------------------------------------------

import os

import numpy as np
import h5py

from argparse import ArgumentParser

### non-standard
from universality import utils
from universality import gaussianprocess as gp
from universality import hyperparams as hp
from universality import plot

#-------------------------------------------------

DEFAULT_MIN = 1e30 ### g/cm^3
DEFAULT_MAX = 1e38

DEFAULT_STITCH_MEAN = 6.0 ### chosen by eye...
DEFAULT_STITCH_PRESSURE = 1e10*utils.c2 ### dyn/cm^2
DEFAULT_STITCH_INDEX = 5

#-------------------------------------------------

parser = ArgumentParser(description=__doc__)

### required arguments
parser.add_argument_group('required arguments')
parser.add_argument('hdf5paths', nargs='+', type=str)

### verbosity options
vgroup = parser.add_argument_group('verbosity arguments')
vgroup.add_argument('-v', '--verbose', default=False, action='store_true')
vgroup.add_argument('-V', '--Verbose', default=False, action='store_true')

### options for stitching
sgroup = parser.add_argument_group('stitching options')
sgroup.add_argument('--stitch', default=False, action='store_true')

sgroup.add_argument('--stitch-pressure-bounds', nargs=2, default=None, type=float,
    help='limit the range of stitching points. DEFAULT is to use --pressure-bounds')
sgroup.add_argument('--stitch-num-points', default=None, type=int,
    help='the number of points to use when constructing the stitching conditions. DEFAULT is to use --num-points')

sgroup.add_argument('--stitch-mean', default=DEFAULT_STITCH_MEAN, type=float,
    help='the mean value for stitching points. \
DEFAULT=%.3f'%DEFAULT_STITCH_MEAN)
sgroup.add_argument('--stitch-pressure', default=DEFAULT_STITCH_PRESSURE, type=float,
    help='the abscissa at which we place a roll-off to enforce stitching. \
We construct a white-noise kernel ~ (p/stitch_pressure)**stitch_index. \
DEFAULT=%.3f'%DEFAULT_STITCH_PRESSURE)
sgroup.add_argument('--stitch-index', default=DEFAULT_STITCH_INDEX, type=float,
    help='the power used to roll-off the white-noise kernel and enforce stitching. \
We construct a white-noise kernel ~ (p/stitch_pressure)**stitch_index. \
DEFAULT=%.3f'%DEFAULT_STITCH_INDEX)

### bounds on hyperparams
parser.add_argument('--poly-degree', default=1, type=int)

parser.add_argument('--min-sigma', default=hp.DEFAULT_MIN_SIGMA, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MIN_SIGMA)
parser.add_argument('--max-sigma', default=hp.DEFAULT_MAX_SIGMA, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MAX_SIGMA)

parser.add_argument('--min-l', default=hp.DEFAULT_MIN_L, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MIN_L)
parser.add_argument('--max-l', default=hp.DEFAULT_MAX_L, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MAX_L)

parser.add_argument('--min-sigma_obs', default=hp.DEFAULT_MIN_SIGMA, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MIN_SIGMA)
parser.add_argument('--max-sigma_obs', default=hp.DEFAULT_MAX_SIGMA, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MAX_SIGMA)

parser.add_argument('--min-m', default=hp.DEFAULT_MIN_M, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MIN_M)
parser.add_argument('--max-m', default=hp.DEFAULT_MAX_M, type=float,
    help='DEFAULT=%.3f'%hp.DEFAULT_MAX_M)

parser.add_argument('--num-grid', default=gp.DEFAULT_NUM, type=int,
    help='the number of grid points in each dimension. \
DEFAULT=%d'%gp.DEFAULT_NUM)

parser.add_argument('--num-mcmc', default=hp.DEFAULT_NUM_MCMC, type=int,
    help='the number of samples to draw in mcmc. \
DEFAULT=%d'%hp.DEFAULT_NUM_MCMC)

parser.add_argument('--num-walkers', default=hp.DEFAULT_NUM_WALKERS, type=int,
    help='DEFAULT=%d'%hp.DEFAULT_NUM_WALKERS)

#parser.add_argument('--maxL-method', default=hp.DEFAULT_METHOD, type=str,
#    help='DEFAULT=%s'%hp.DEFAULT_METHOD)
#
#parser.add_argument('--maxL-tol', default=hp.DEFAULT_TOL, type=float,
#    help='DEFAULT=%s'%hp.DEFAULT_TOL)

parser.add_argument('--l-prior', default=hp.DEFAULT_L_PRIOR, type=str)
parser.add_argument('--sigma-prior', default=hp.DEFAULT_SIGMA_PRIOR, type=str)
parser.add_argument('--sigma_obs-prior', default=hp.DEFAULT_SIGMA_PRIOR, type=str)
parser.add_argument('--m-prior', default=hp.DEFAULT_M_PRIOR, type=str)

parser.add_argument('--temperature', default=hp.DEFAULT_TEMPERATURE, type=float,
    help='the temperature used to flatten the likelihood a la parallel tempering. \
DEFAULT=%.3f'%hp.DEFAULT_TEMPERATURE)

parser.add_argument('--grid', default=False, action='store_true')
parser.add_argument('--mcmc', default=False, action='store_true')
#parser.add_argument('--maxL', default=False, action='store_true')

parser.add_argument('--strip-mcmc', default=0, type=int,
    help='the number of burn in samples to reject from the mcmc sampler')

parser.add_argument('--num-proc', default=utils.DEFAULT_NUM_PROC, type=int)

parser.add_argument('-p', '--plot', default=False, action='store_true')

parser.add_argument('--log-sigmas', default=False, action='store_true')
parser.add_argument('--include-logL', default=False, action='store_true')

parser.add_argument('-o', '--output-dir', default='.', type=str)
parser.add_argument('-t', '--tag', default='', type=str)
parser.add_argument('--figtype', default=[], type=str, action='append')
parser.add_argument('--dpi', default=plot.DEFAULT_DPI, type=float)

args = parser.parse_args()

#assert np.any((args.grid, args.mcmc, args.maxL)), 'please specify at least one of: --grid, --mcmc, --maxL'
assert np.any((args.grid, args.mcmc)), 'please specify at least one of: --grid, --mcmc'

### finish parsing
Nhdf5 = len(args.hdf5paths)

if args.stitch_pressure_bounds is None:
    args.stitch_pressure_bounds = args.pressure_bounds
if args.stitch_num_points is None:
    args.stitch_num_points = args.num_points

if not os.path.exists(args.output_dir):
    os.makedirs(args.output_dir)

if args.tag:
    args.tag = "_"+args.tag

if not args.figtype:
    args.figtype = plot.DEFAULT_FIGTYPES

args.verbose |= args.Verbose

#-------------------------------------------------

models = []
for hdf5path in args.hdf5paths:
    if args.verbose:
        print('reading: '+hdf5path)
    model = gp.hdf5load(hdf5path)
    if args.Verbose:
        if len(model)==1:
            print('    found mixture model with 1 element')
        else:
            print('    found mixture model with %d elements'%len(model))
    models.append(model)

### create combinatorically many possible matchings between all models
inds = zip(*[_.flatten() for _ in np.meshgrid(*[range(len(model)) for model in models])])
Ninds = len(inds)

#-------------------------------------------------

### add stitching stuff
if args.stitch: ### set up stitching conditions
    if args.verbose:
        print('enforcing stitching to f=%.3e with pressure-scale=%.3e dyn/cm^2 and index=%.3f at %d points within  [%.3e, %.3e] dyn/cm^2'%\
            ((args.stitch_mean, args.stitch_pressure, args.stitch_index, args.stitch_num_points)+tuple(args.stitch_pressure_bounds)))
    x_stitch = np.linspace(
        np.log(args.stitch_pressure_bounds[0]),
        np.log(args.stitch_pressure_bounds[1]),
        args.stitch_num_points,
    )
    x_stitch -= 2*np.log(utils.c) ### divide by c^2 to get this into consistent units with gpr-resample, etc
    f_stitch = np.ones(args.stitch_num_points, dtype=float)*args.stitch_mean
    cov_stitch = np.diag(np.exp(x_stitch - np.log(args.stitch_pressure/utils.c2))**args.stitch_index) ### the stitching white-noise kernel

#------------------------

#--- now do some logLikelihood stuff!

if args.plot:
    overlayfig = None

    if args.log_sigmas:
        labels = ['$\log_{10}\sigma$', '$l$', '$\log_{10}\sigma_\mathrm{obs}$', '$\log_{10}m$']
    else:
        labels = ['$\sigma$', '$l$', '$\sigma_\mathrm{obs}$', '$m$']
    if args.include_logL:
        labels.append('$\log\mathcal{L}$')
    labels = np.array(labels)

    include = np.ones(5 if args.include_logL else 4, dtype=bool)
    if args.min_sigma==args.max_sigma:
        include[0] = False
    if args.min_l==args.max_l:
        include[1] = False
    if args.min_sigma_obs==args.max_sigma_obs:
        include[2] = False
    if args.min_m==args.max_m:
        include[3] = False

truths = None

#---

if args.grid:
    if args.verbose:
        print('evaluating on a %s grid'%("x".join('%d'%args.num_grid for _ in xrange(np.sum(include)-args.include_logL))))

    sigma = hp.param_grid(args.min_sigma, args.max_sigma, size=args.num_grid, prior=args.sigma_prior)
    length_scale = hp.param_grid(args.min_l, args.max_l, size=args.num_grid, prior=args.l_prior)
    sigma_obs = hp.param_grid(args.min_sigma_obs, args.max_sigma_obs, size=args.num_grid, prior=args.sigma_obs_prior)
    model_multiplier = hp.param_grid(args.min_m, args.max_m, size=args.num_grid, prior=args.m_prior)

    SIGMA, LENGTH_SCALE, SIGMA_OBS, MODEL_MULTIPLIER = np.meshgrid(sigma, length_scale, sigma_obs, model_multiplier, indexing='ij')
    SIGMA = SIGMA.flatten()
    LENGTH_SCALE = LENGTH_SCALE.flatten()
    SIGMA_OBS = SIGMA_OBS.flatten()
    MODEL_MULTIPLIER = MODEL_MULTIPLIER.flatten()

    ### perform the actual iteration, which will be expensive as all get out
    logLikes = []
    tmp = '    %'+str(int(np.floor(np.log10(len(SIGMA)))))+'d / '+'%d'%len(SIGMA)
    for ind, (s, l, S, m) in enumerate(zip(SIGMA, LENGTH_SCALE, SIGMA_OBS, MODEL_MULTIPLIER)):
        if args.Verbose:
            print(tmp%(ind+1))

        s2 = s**2
        l2 = l**2
        S2 = S**2

        logweights = [] ### holder for all weights before we normalize them
        for indecies in inds:
            ### iterate over the models included here and extract the relevant parameters for this combination of models
            if args.Verbose:
                print('        extracting data from each model included in this combination')
            x_obs = []
            f_obs = []
            xlabel = models[0][0]['labels']['xlabel']
            flabel = models[0][0]['labels']['flabel']
            for model, j in zip(models, indecies):
                assert xlabel==model[j]['labels']['xlabel'], 'xlabel must match for all models!'
                assert flabel==model[j]['labels']['flabel'], 'flabel must match for all models!'
                x_obs.append(model[j]['x'])
                f_obs.append(model[j]['f'])
            x_obs = np.concatenate(x_obs)
            f_obs = np.concatenate(f_obs)

            ### compute the big uncertainty estimate between all the models
            Nobs = len(x_obs)

            ### add in "theory model noise" as diagonal components based on variance of means at each pressure
            if args.Verbose:
                print('        assembling measurement uncertainty covariance')
            if args.stitch:
                covs = np.zeros((Nobs+args.stitch_num_points, Nobs+args.stitch_num_points), dtype=float) ### include space for the stitching conditions

                x_obs = np.concatenate((x_obs, x_stitch)) ### add in stitching data
                f_obs = np.concatenate((f_obs, f_stitch))
                covs[Nobs:,Nobs:] = cov_stitch

            else:
                covs = np.zeros((Nobs,Nobs), dtype=float)

            ### add block-diagonal components
            if args.Verbose:
                print('        adding block-diagonal contributions')
            start = 0
            for model, j in zip(models, indecies):
                stop = start+len(model[j]['x']) ### the number of points in this model
                covs[start:stop,start:stop] = model[j]['cov'] ### fill in block-diagonal component
                start = stop

            ### iterate through pressure samples and compute theory variance of each
            if args.verbose:
                print('        adding modeling uncertainty at each x-value')

            ### NOTE: the following iteration may not be the most efficient thing in the world, but it should get the job done...
            truth = np.empty(covs.shape[0], dtype=bool) ### used to index coveriance matrix when filling things in
            for x in set(x_obs): # iterate over all included x-points

                truth[:] = False ### re-set this

                sample = [] # set up holders for the values from each model that covers this pressure point
                sample_covs = []

                ### iterate over each model, checking to see whether it has this x-value
                start = 0
                for model, j in zip(models, indecies):
                    n = len(model[j]['x'])
                    i = np.arange(n)[x==model[j]['x']]
                    if len(i): ### there's a match
                        i = i[0]
                        truth[start+i] = True ### we update this index in the big covariance matrix
                        sample.append(model[j]['f'][i]) ### record the mean
                        sample_covs.append(model[j]['cov'][i,i]) ### and the covariance
                    start += n ### bump this

                ### add the sample variance for this pressure mulitplied by some factor (hyper-parameter)
                covs[truth,truth] += (np.var(sample) + np.sum(sample_covs))*m

            ### add the squared-exponential covariance kernel
            covs += gp._cov(x_obs, sigma2=s2, l2=l2, sigma2_obs=S2)

            #-------------------------------------------------

            ### compute likelihood
            if args.Verbose:
                print('        computing logLike for this set of conditioned values and hyperparams')
            f_fit, _ = gp.poly_model(np.array([]), f_obs, x_obs, degree=args.poly_degree)
            logweight = gp._logLike(f_obs-f_fit, np.linalg.inv(covs)) ### compute logLike for seeing the data on which we condition

            ### include the prior weights from each mixture model
            for model, j in zip(models, indecies):
                logweight += np.log(model[j]['weight'])

            logweights.append(logweight)

        ### sum over models in the mixture to compute the logLike of this whole thing
        if len(logweights) == 1: ### no need to sum
            logLikes.append(logweights[0])

        else: # sum over components of the mixture model
            logweights = np.array(logweights)
            maximum = np.max(logweights)
            logLikes.append(np.log(np.sum(np.exp(logweights-maximum)))+maximum) ### sum over models to get the overall loglike

    logLikes = np.array(logLikes)*(1./args.temperature) ### flatten likelihoods with an inverse temperature a la parallel tempering

    ### write the results to disk
    path = "%s/investigate-gpr-gpr-hyperparams-grid%s.csv"%(args.output_dir, args.tag)
    if args.verbose:
        print('writing: '+path)
    tmp = '%d,'%args.poly_degree + ','.join('%.9e' for _ in xrange(5)) # s, l, S, m, logLike
    with open(path, 'w') as file_obj:
        print >> file_obj, 'poly_degree,sigma,l,sigma_obs,multiplier,logLike'
        for tup in zip(SIGMA, LENGTH_SCALE, SIGMA_OBS, MODEL_MULTIPLIER, logLikes):
            print >> file_obj, tmp%tup

    if args.plot:
        if args.include_logL:
            if args.log_sigmas:
                data = np.transpose([np.log10(SIGMA), LENGTH_SCALE, np.log10(SIGMA_OBS), np.log10(MODEL_MULTIPLIER), logLikes])
            else:
                data = np.transpose([SIGMA, LENGTH_SCALE, SIGMA_OBS, MODEL_MULTIPLIER, logLikes])
        else:
            if args.log_sigmas:
                data = np.transpose([np.log10(SIGMA), LENGTH_SCALE, np.log10(SIGMA_OBS), np.log10(MODEL_MULTIPLIER)])
            else:
                data = np.transpose([SIGMA, LENGTH_SCALE, SIGMA_OBS, MODEL_MULTIPLIER])

        weights = np.exp(logLikes-np.max(logLikes))
        weights /= np.sum(weights)

#        truth = weights>1e-2/len(weights)
        truth = np.ones(len(weights), dtype=bool)

        if args.verbose:
            print('    plotting')

        fig = plot.corner(
            data[truth][:,include],
            labels=labels[include],
            weights=weights[truth],
            truths=truths[include] if truths is not None else None,
            color='b',
        )
        plot.save('investigate-gpr-gpr-hyperparams-grid%s'%args.tag, fig, directory=args.output_dir, figtypes=args.figtype, dpi=args.dpi, verbose=args.verbose)
        plot.plt.close(fig)

        ### add to overlay plot
        overlayfig = plot.corner(
            data[truth][:,include],
            labels=labels[include],
            weights=weights[truth],
            color='b',
            truths=truths[include] if truths is not None else None,
            fig=overlayfig,
        )

#---

if args.mcmc:
    if args.verbose:
        print('running MCMC for %d steps with %d walkers'%(args.num_mcmc, args.num_walkers))
    raise NotImplementedError

#--- wrap up overlay plot

if args.plot and (overlayfig is not None):
    plot.save('investigate-gpr-gpr-hyperparams%s'%args.tag, overlayfig, directory=args.output_dir, figtypes=args.figtype, dpi=args.dpi, verbose=args.verbose)
    plot.plt.close(overlayfig)
