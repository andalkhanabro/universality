#!/usr/bin/env python

__usage__ = "gpr_foo [--options] x_column y_column x_test.csv eos.csv [eos.csv eos.csv ...]"
__doc__ = "a script that compute the conditioned Gaussian process given the set of observed data"
__author__ = "reed.essick@ligo.org"

#-------------------------------------------------

import os

import numpy as np

from optparse import OptionParser

### non-standard libraries
from universality import utils
from universality import gaussianprocess as gp

#-------------------------------------------------

parser = OptionParser(usage=__usage__, description=__doc__)

parser.add_option('-v', '--verbose', default=False, action='store_true')

#--- data pre-conditioning
parser.add_option('', '--log-x', default=False, action='store_true',
    help='fit using log(x_column) instead of x_column')
parser.add_option('', '--log-y', default=False, action='store_true',
    help='fit using log(y_column) instead of y_column')

parser.add_option('', '--poly-degree', default=1, type='int',
    help='the order of the polynomial fit used to model data before feeding to GP subroutines. \
This essentially serves as the "mean" of the fit about which the GP models variations. \
DEFAULT=1')

parser.add_option('', '--min-x', default=-np.infty, type='float',
    help='only use data above this value. \
DEFAULT=-inf')
parser.add_option('', '--max-x', default=+np.infty, type='float',
    help='only use data below this value. \
DEFAULT=+inf')

parser.add_option('', '--min-y', default=-np.infty, type='float',
    help='only use data above this value. \
DEFAUL=+inf')
parser.add_option('', '--max-y', default=+np.infty, type='float',
    help='only use data below this value. \
DEFAUL=+inf')

#--- sampling logic
### FIXME: 
###   want to figure out a way to fairly resample the data so that each EOS is equally weighted in the resulting process?
###   If one EOS has more entries in it's table than another, we should not weight it more!
parser.add_option('', '--resample-input', default=False, action='store_true',
    help='NOT IMPLEMENTED')

#--- Gaussian Process hyper-parameters
parser.add_option('', '--sigma', default=gp.__default_sigma__, type='float',
    help='hyperparameter for Gaussian Process. \
DEFAULT=%.3e'%gp.__default_sigma__)
parser.add_option('', '--l', default=gp.__default_l__, type='float',
    help='hyperparameter for Gaussian Process. \
DEFAULT=%.3e'%gp.__default_l__)
parser.add_option('', '--sigma_obs', default=gp.__default_sigma__, type='float',
    help='hyperparameter for Gaussian Process. \
DEFAULT=%.3e'%gp.__default_sigma__)

#--- output formatting
parser.add_option('-o', '--output-dir', default='.', type='string')
parser.add_option('-t', '--tag', default='', type='string')

opts, args = parser.parse_args()
assert len(args)>3, 'please supply at least 4 input arguments\n%s'%__usage__
xcol, ycol = columns = args[:2]
xtestpath = args[2]
paths = args[3:]

if not os.path.exists(opts.output_dir):
    os.makedirs(opts.output_dir)

if opts.tag:
    opts.tag = "_"+opts.tag

#-------------------------------------------------

### read in data and add it to a cumulative (unordered) set
if opts.verbose:
    print('reading in eos data')
Xobs = np.array([])
Yobs = np.array([])
for path in paths:
    if opts.verbose:
        print('reading: '+path)
    data, _ = utils.load(path, columns)
    x = data[:,0]
    y = data[:,1]

    ### FIXME: need to insert some logic here...
    if opts.resample_input:
        raise NotImplementedError, 'need to set up some logic to resample the input so each EOS has equal representation in the resulting GP (equal number of input data points'

    truth = (opts.min_x<=x)*(x<=opts.max_x)*(opts.min_y<=y)*(y<=opts.max_y)
    if np.any(truth):
        Xobs = np.concatenate((Xobs, x[truth]))
        Yobs = np.concatenate((Yobs, y[truth]))
    else:
        if opts.verbose:
            print('    no data found with x in [%.3e, %.3e] and y in [%.3e, %.3e]! skipping this eos'%(opts.min_x, opts.max_x, opts.min_y, opts.max_y))
        continue ### skip this eos

### read in the evaluation points from x_test.csv
if opts.verbose:
    print('reading test points from: '+xtestpath)
Xtest, _ = utils.load(xtestpath, [xcol])
Xtest = Xtest[:,0]

#-------------------------------------------------

### transform data as needed
if opts.verbose:
    print('precondidtioning data')

# take logs of data if requested
labels = []
if opts.log_x:
    Xobs = np.log(Xobs)
    labels.append('log'+xcol)
else:
    labels.append(xcol)

if opts.log_y:
    Yobs = np.log(Yobs)
    labels.append('log'+ycol)
else:
    labels.append(ycol)

if opts.log_x:
    Xtest = np.log(Xtest)

# fit a polynomial to the data
poly = np.polyfit(Xobs, Yobs, opts.poly_degree)
Y_mean = np.zeros_like(Yobs, dtype='float')
Y_test = np.zeros_like(Xtest, dtype='float')
for i in xrange(opts.poly_degree+1):
    Y_mean += poly[-1-i]*Xobs**i
    Y_test += poly[-1-i]*Xtest**i

#-------------------------------------------------

### now, GPR the data!
if opts.verbose:
    print('computing conditioned process for %s as a function of %s'%tuple(labels[::-1]))
    print('using %d observed points and %d test points'%(len(Xobs), len(Xtest)))
mean, cov = gp.gpr_f(Xtest, Yobs-Y_mean, Xobs, sigma2=opts.sigma**2, l2=opts.l**2, sigma2_obs=opts.sigma_obs**2)
mean += Y_test ### add back in the mean you've subtracted

pklpath = os.path.join(opts.output_dir, 'gpr_foo-%s-%s%s.pkl'%tuple(labels+[opts.tag]))
if opts.verbose:
    print('writing process to: '+pklpath)
gp.pkldump(pklpath, labels[0], labels[1], Xtest, mean, cov)
