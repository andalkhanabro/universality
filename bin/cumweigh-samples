#!/usr/bin/env python

__doc__ = "a script that computes the associated weights for target_samples.csv based on the distribution within source_samples.csv. This assumes a 1-Dimensional distribution within source_samples.csv and applies weights based on the cumulative distribution observed therein"
__author__ = "reed.essick@ligo.org"

#-------------------------------------------------

import numpy as np

from argparse import ArgumentParser

### non-standard libraries
from universality import utils

#-------------------------------------------------

parser = ArgumentParser(description=__doc__)

parser.add_argument('source', type=str)
parser.add_argument('target', type=str)
parser.add_argument('output', type=str)

parser.add_argument('source_column', type=str,
    help='the column name within source_samples.csv')
parser.add_argument('target_column', type=str,
    help='the column name in target_samples.csv up to which we integrate the source sample\'s cumulative distribution to obtain the weight')
parser.add_argument('prior_bounds', nargs=2, type=float,
    help='the prior bound used to complete the integral.')

parser.add_argument('--cumulative-integral-direction', default=utils.DEFAULT_CUMULATIVE_INTEGRAL_DIRECTION, type=str,
    help='must be one of: %s'%(', '.join(utils.KNOWN_CUMULATIVE_INTEGRAL_DIRECTIONS)))

parser.add_argument('-v', '--verbose', default=False, action='store_true')

parser.add_argument('--logcolumn-source', default=False, action='store_true',
    help='convert the values read in for this column to natural log.')
parser.add_argument('--logcolumn-target', default=False, action='store_true',
    help='convert the values read in for this column to natural log.')

parser.add_argument('--weight-column', default=[], type=str, action='append',
    help='if provided, thie numerical values from this column will be used as weights in the KDE')
parser.add_argument('--weight-column-is-log', default=[], type=str, action='append',
    help='if supplied, interpret the values in weight_column as log(weight), meaning we exponentiate them before using them in the KDE')
parser.add_argument('--invert-weight-column', default=[], type=str, action='append',
    help='After extracting the weights from source_samples.csv, this will compute the KDE using \
the inverse of those values; e.g.: weight by the inverse of the prior for a set of posterior \
samples so the effective sampling is with respect to the likelihood. The inversion is done after \
exponentiation when --weight-column-is-log is supplied.')

parser.add_argument('--output-weight-column', default=utils.DEFAULT_WEIGHT_COLUMN, type=str,
    help='the name of the new weight-column in the output file. **BE CAREFUL!** You should make sure this \
is consistent with whether or not you specified --do-not-log-output-weight! \
DEFAULT='+utils.DEFAULT_WEIGHT_COLUMN)
parser.add_argument('--do-not-log-output-weights', default=False, action='store_true',
    help='record the raw weights instead of the log(weight) in the output CVS. **BE CAREFUL!** You should make \
sure this is consistent with the name specified by --output-weight-column.')

parser.add_argument('--num-proc', default=utils.DEFAULT_NUM_PROC, type=int,
    help='number of processes for parallelized computation of logkde. \
DEFAULT=%d'%utils.DEFAULT_NUM_PROC)

args = parser.parse_args()

assert args.cumulative_integral_direction in utils.KNOWN_CUMULATIVE_INTEGRAL_DIRECTIONS, \
    '--cumulative-integral-direction=%s not understood! must be one of: %s'%(', '.join(utils.KNOWN_CUMULATIVE_INTEGRAL_DIRECTIONS))

args.logcolumn_source = [args.source_column] if args.logcolumn_source else []
args.logcolumn_target = [args.source_target] if args.logcolumn_target else []

#-------------------------------------------------

### read in source samples
if args.verbose:
    print('reading source samples from: '+args.source)
srcdata, columns = utils.load(args.source, [args.source_column], logcolumns=args.logcolumn_source)

if args.weight_column:
    if args.verbose:
        print('reading in non-trivial weights from: '+args.source)
    weights = utils.load_weights(args.source, args.weight_column, logweightcolumns=args.weight_column_is_log, invweightcolumns=args.invert_weight_column)

else:
    N = len(srcdata)
    weights = np.ones(N, dtype=float)/N

### prune source by prior bounds
if args.verbose:
    print('pruning source samples so they lie within [%.3f, %.3f)'%tuple(args.prior_bounds))
N = len(srcdata)
srcdata, weights = utils.prune(srcdata, [args.prior_bounds], weights=weights)
weights /= np.sum(weights)
if args.verbose:
    print('retained %d / %d samples'%(len(srcdata), N))

srcdata = srcdata.flatten() ### make this a 1-D array

#------------------------

### read in target samples
if args.verbose:
    print("reading in target samples from: "+args.target)
tgtdata, tgtcolumns = utils.load(args.target, logcolumns=args.logcolumn_target) ### load in all the columns!
utils.check_columns(tgtcolumns, [args.target_column]) ### make sure we have the columns we need to

#-------------------------------------------------

if args.verbose:
    print('computing weights from cumulative distribution at %d samples from %s based on %d samples from %s with %d cores'%\
        (len(tgtdata), args.target, len(srcdata), args.source, args.num_proc))
logcdf = utils.logcdf(
    tgtdata[:,tgtcolumns.index(args.target_column)],
    srcdata,
    args.prior_bounds,
    weights=weights,
    direction=args.cumulative_integral_direction,
    num_proc=args.num_proc
)
if args.do_not_log_output_weights:
    logcdf = np.exp(logcdf)

#------------------------------------------------

if args.verbose:
    print('writing results with weight-column=%s into: %s'%(args.output_weight_column, args.output))

template = ','.join('%.9e' for _ in xrange(len(tgtcolumns)+1))
with open(args.output, 'w') as file_obj:
    print >> file_obj, ','.join(tgtcolumns+[args.output_weight_column])
    for sample, logweight in zip(tgtdata, logcdf):
        print >> file_obj, template%tuple(list(sample)+[logweight])
