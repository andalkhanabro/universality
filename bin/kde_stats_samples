#!/usr/bin/env python

__doc__ = "generate statistics based on the KDE over these samples"
__usage__ = "kde_stats_samples [--options] samples.csv column1 [column2 column3...]"
__author__ = "reed.essick@ligo.org"

#-------------------------------------------------

import numpy as np

from optparse import OptionParser

### non-standard libraries
from universality import utils
from universality import stats
from universality import plot

#-------------------------------------------------

parser = OptionParser(usage=__usage__, description=__doc__)

parser.add_option('-v', '--verbose', default=False, action='store_true')

parser.add_option('', '--logcolumn', default=[], type='string', action='append',
    help='convert the values read in for this column to natural log. \
Can be repeated to specify multiple columns. \
DEFAULT=[]')

parser.add_option('', '--range', nargs=3, default=[], action='append', type='string',
    help='specify the ranges used in corner.corner (eg.: "--range column minimum maximum"). \
Can specify ranges for multiple columns by repeating this option. \
DEFAULT will use the minimum and maximum observed sample points.')

parser.add_option('--max-num-samples', default=utils.DEFAULT_MAX_NUM_SAMPLES, type='int')
parser.add_option('', '--weight-column', default=None, type='string',
    help='if provided, thie numerical values from this column will be used as weights in the KDE')
parser.add_option('', '--weight-column-is-log', default=False, action='store_true',
    help='if supplied, interpret the values in weight_column as log(weight), meaning we exponentiate them before using them in the KDE')

parser.add_option('--num-points', default=plot.DEFAULT_NUM_POINTS, type='int',
    help='DEFAULT=%d'%plot.DEFAULT_NUM_POINTS)
parser.add_option('', '--bandwidth', nargs=2, default=[], type='string', action='append',
    help='the bandwidths used for each column specified. We assume diagonal covariance matricies in the Gaussian kernel. \
If you do not specify a bandwidth for a column, the default value (%.3f) will be used.'%utils.DEFAULT_BANDWIDTH)
parser.add_option('', '--reflect', default=False, action='store_true',
    help='reflect the points about their boundaries within the KDE')
parser.add_option('', '--prune', default=False, action='store_true',
    help='throw away samples that fall outside ranges')

### options about what to compute
parser.add_option('--confidence-region', default=[], type='float', action='append',
    help='compute the confidence region volume for this confidence level [0.0, 1.0]. Can be repeated.')
parser.add_option('--entropy', default=False, action='store_true')
parser.add_option('--information', default=False, action='store_true')

parser.add_option('--argmax', default=False, action='store_true')
parser.add_option('--dlogL', default=[], type='string', action='append',
    help='comma separated list of the parameter values used within dlogL. Can be repeated.')
parser.add_option('--dtheta', default=[], type='string', action='append',
    help='comma separated list of the parameter values used within dtheta. Can be repeated.')

opts, args = parser.parse_args()
assert len(args)>1, 'please supply at least 2 input argument\n%s'%__usage__
inpath = args[0]
columns = args[1:]
Ncol = len(columns)

rangesdict = dict((column,(float(_min), float(_max))) for column, _min, _max in opts.range)

opts.dlogL = [[float(_) for _ in v.split(',')] for v in opts.dlogL]
opts.dtheta = [[float(_) for _ in v.split(',')] for v in opts.dtheta]

bandwidthdict = dict((col, float(val)) for col, val in opts.bandwidth)
variances = np.array([bandwidthdict.get(col, utils.DEFAULT_BANDWIDTH) for col in columns])**2

#-------------------------------------------------

### read in data from csv
if opts.verbose:
    print('reading samples from: '+inpath)
data, columns = utils.load(inpath, columns, logcolumns=opts.logcolumn, max_num_samples=opts.max_num_samples)

ranges = []
for i, col in enumerate(columns):
    if rangesdict.has_key(col):
        ranges.append(rangesdict[col])
    else:
        ranges.append((np.min(data[:,i]), np.max(data[:,i])))

if opts.weight_column!=None:
    if opts.verbose:
        print('reading in non-trivial weights from: '+inpath)
    weights, _ = utils.load(inpath, [opts.weight_column], max_num_samples=opts.max_num_samples)
    weights = weights.reshape((len(weights)))

    if opts.weight_column_is_log:
        weights = np.exp(weights-np.max(weights))
    weights /= np.sum(weights)

else:
    N = len(data)
    weights = np.ones(N, dtype='float')/N

#------------------------

if opts.verbose:
    print('computing kde')
ranges = [utils.data2range(data[:,i]) for i in xrange(Ncol)]
vects = [np.linspace(m, M, opts.num_points) for m, M in ranges]

if opts.prune:
    data, weights = utils.prune(data, ranges, weights=weights)
if opts.reflect:
    data, weights = utils.reflect(data, ranges, weights=weights)
logkde = utils.logkde(
    utils.vects2flatgrid(*vects),
    data,
    variances,
    weights=weights,
)

if opts.verbose:
    print('computing statistics')

if opts.confidence_region:
    opts.confidence_region.sort()
    for level, vol in zip(opts.confidence_region, stats.logkde2cr(vects, logkde, opts.confidence_region)):
        print('Volume(CR=%.3f) = %.6e'%(level, vol))

if opts.entropy:
    print('H = %.6e'%stats.logkde2entropy(vects, logkde))

if opts.information:
    print('I = %.6e'%stats.logkde2information(vects, logkde))

if opts.argmax:
    print('argmax = %s'%(stats.logkde2argmax(vects, logkde)))

if opts.dlogL:
    maxlogL = np.max(logkde)
    for point in opts.dlogL:
        print('dlogL(%s) = %.6e'%(point, utils.logkde(np.array([point]), data, variances, weights=weights) - maxlogL))

if opts.dtheta:
    for point in opts.dtheta:
        print('dtheta(%s) = %.6e'%(point, stats.dtheta(point, vects, logkde)))
