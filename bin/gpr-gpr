#!/usr/bin/env python3

"""read in existing processes from disk and generate another process over them.
"""
__author__ = "Reed Essick (reed.essick@gmail.com)"

#---------------------------------------------------------------------------------------------------

import os

import numpy as np
import h5py

from argparse import ArgumentParser

### non-standard
from universality import utils
from universality.gaussianprocess import gaussianprocess as gp
from universality.gaussianprocess import hyperparameters as hp
from universality import plot

#-------------------------------------------------

DEFAULT_MIN = 1e30 ### g/cm^3
DEFAULT_MAX = 1e38

DEFAULT_STITCH_MEAN = 6.0 ### chosen by eye...
DEFAULT_STITCH_PRESSURE = 1e10*utils.c2 ### dyn/cm^2
DEFAULT_STITCH_INDEX = 5

#-------------------------------------------------

parser = ArgumentParser(description=__doc__)

### required arguments
parser.add_argument_group('required arguments')
parser.add_argument('hdf5paths', nargs='+', type=str)

### verbosity options
vgroup = parser.add_argument_group('verbosity arguments')
vgroup.add_argument('-v', '--verbose', default=False, action='store_true')
vgroup.add_argument('-V', '--Verbose', default=False, action='store_true')

### options for evaluation
ggroup = parser.add_argument_group('Gaussian Process options')
ggroup.add_argument('--pressure-bounds', default=(DEFAULT_MIN, DEFAULT_MAX), nargs=2, type=float,
    help='min max values for evaluation bounds. Specified in the same units used in the supplied pkl. \
DEFAULT=%.3e %.3e'%(DEFAULT_MIN, DEFAULT_MAX))
ggroup.add_argument('-n', '--num-points', default=gp.DEFAULT_NUM, type=int,
    help='evaluate at this number of points. \
DEFAULT=%d'%gp.DEFAULT_NUM)

ggroup.add_argument('--external-process', default=[], type=str, action='append',
    help='if supplied, add this process as a white-noise term before conditioning. \
We should be able to make it such that the final process has this mean and covariance within the pressures of interest while still smoothly stitching on to the stuff that is conditioned on the other models. Can be repeated to specify mulitple external processes')

ggroup.add_argument('--hyperparampath', default=None, type=str,
    help='a path to a csv with the following columns: "polydeg", "sigma", "l", "sigma_obs", "loglike" such as is produced by investigate-hyperparams. \
If supplied, we ignore values of --sigma, --length-scale, --sigma-obs and build a mixture-model hdf5 file based on the weights contained in the csv.')
ggroup.add_argument('--max-num-models', default=None, type=int,
    help='if supplied, only include up to this many models in the mixture, selecting those with the largest weights first')

ggroup.add_argument('--poly-degree', default=gp.DEFAULT_POLY_DEGREE, type=int,
    help='the degree of the polynomial used to model eos before GPR as part of evaluation. \
DEFAULT=%d'%gp.DEFAULT_POLY_DEGREE)

ggroup.add_argument('-s', '--sigma', default=gp.DEFAULT_SIGMA, type=float,
    help='used as a guess for the optimizer. \
DEFAULT=%.3e'%gp.DEFAULT_SIGMA)
ggroup.add_argument('-l', '--length-scale', dest='l', default=gp.DEFAULT_L, type=float,
    help='used as a guess for the optimizer. \
DEFAULT=%.3e'%gp.DEFAULT_L)
ggroup.add_argument('-S', '--sigma-obs', default=gp.DEFAULT_SIGMA, type=float,
    help='used as a guess for the optimizer. \
DEFAULT=%.3e'%gp.DEFAULT_SIGMA)

ggroup.add_argument('-m', '--model-multiplier', default=gp.DEFAULT_MODEL_MULTIPLIER, type=float,
    help='multiplicative factor for theoretical variance. Larger values increase the "theory noise" from the variance between resampled curves. \
Default=%d'%gp.DEFAULT_MODEL_MULTIPLIER)
parser.add_argument('--diagonal-model-covariance', default=False, action='store_true')

ggroup.add_argument('--temperature', default=hp.DEFAULT_TEMPERATURE, type=float,
    help='the temperature applied to the weights after conditioning.')

### options for stitching
sgroup = parser.add_argument_group('stitching options')
sgroup.add_argument('--stitch', default=False, action='store_true')

sgroup.add_argument('--stitch-pressure-bounds', nargs=2, default=None, type=float,
    help='limit the range of stitching points. DEFAULT is to use --pressure-bounds')
sgroup.add_argument('--stitch-num-points', default=None, type=int,
    help='the number of points to use when constructing the stitching conditions. DEFAULT is to use --num-points')

sgroup.add_argument('--stitch-mean', default=DEFAULT_STITCH_MEAN, type=float,
    help='the mean value for stitching points. \
DEFAULT=%.3f'%DEFAULT_STITCH_MEAN)
sgroup.add_argument('--stitch-pressure', default=DEFAULT_STITCH_PRESSURE, type=float,
    help='the abscissa at which we place a roll-off to enforce stitching. \
We construct a white-noise kernel ~ (p/stitch_pressure)**stitch_index. \
DEFAULT=%.3f'%DEFAULT_STITCH_PRESSURE)
sgroup.add_argument('--stitch-index', default=DEFAULT_STITCH_INDEX, type=float,
    help='the power used to roll-off the white-noise kernel and enforce stitching. \
We construct a white-noise kernel ~ (p/stitch_pressure)**stitch_index. \
DEFAULT=%.3f'%DEFAULT_STITCH_INDEX)

### plotting options
pgroup = parser.add_argument_group('plotting options')
pgroup.add_argument('-p', '--plot', default=False, action='store_true')
pgroup.add_argument('--level', default=[], type=float, action='append',
    help='the confidence levels used within plot. Can be repeated to specify multiple levels')
pgroup.add_argument('--fractions', default=False, action='store_true')
pgroup.add_argument('--residuals', default=False, action='store_true')
pgroup.add_argument('--ratios', default=False, action='store_true')

pgroup.add_argument('--ylabel', default='$\phi$', type=str)

pgroup.add_argument('--figwidth', default=plot.DEFAULT_FIGWIDTH, type=float)
pgroup.add_argument('--figheight', default=plot.DEFAULT_FIGHEIGHT, type=float)

pgroup.add_argument('--grid', default=False, action='store_true')

### output options
ogroup = parser.add_argument_group('output options')
ogroup.add_argument('-o', '--output-dir', default='.', type=str)
ogroup.add_argument('-t', '--tag', default='', type=str)
ogroup.add_argument('--figtype', default=[], type=str, action='append')
ogroup.add_argument('--dpi', default=plot.DEFAULT_DPI, type=float)

args = parser.parse_args()

### finish parsing
Nhdf5 = len(args.hdf5paths)

if args.stitch:
    if args.stitch_pressure_bounds is None:
        args.stitch_pressure_bounds = args.pressure_bounds
    if args.stitch_num_points is None:
        args.stitch_num_points = args.num_points

if args.output_dir and (not os.path.exists(args.output_dir)):
    os.makedirs(args.output_dir)

if args.tag:
    args.tag = "_"+args.tag

if not args.figtype:
    args.figtype = plot.DEFAULT_FIGTYPES

if not args.level:
    args.level = plot.DEFAULT_LEVELS

args.verbose |= args.Verbose

#-------------------------------------------------

# figure out the hyperparameters we'll use
if args.hyperparampath:
    polydeg, sigma, length_scale, sigma_obs, model_multiplier, weights = utils.load(args.hyperparampath, ['poly_degree', 'sigma', 'l', 'sigma_obs', 'multiplier', 'logLike'])[0].transpose()
    polydeg = polydeg.astype(int)

    if args.max_num_models is not None: ### truncate the number of models considered
        truth = weights.argsort()[::-1][:args.max_num_models]
        polydeg = polydeg[truth]
        sigma = sigma[truth]
        length_scale = length_scale[truth]
        sigma_obs = sigma_obs[truth]
        weights = weights[truth]

    weights = np.exp(weights-np.max(weights))
    weights /= np.sum(weights)

    truth = weights > 0 ### throw away anything that won't ever matter
    polydeg = polydeg[truth]
    sigma = sigma[truth]
    length_scale = length_scale[truth]
    sigma_obs = sigma_obs[truth]
    weights = weights[truth]

else:
    polydeg = [args.poly_degree]
    sigma = [args.sigma]
    length_scale = [args.l]
    sigma_obs = [args.sigma_obs]
    model_multiplier = [args.model_multiplier]
    weights = [1.]

#-------------------------------------------------

### set up the evaluation points, which are use repeatedly within the following loop
if args.verbose:
    print('evaluating f at %d points within [%.3e, %.3e] dyn/cm^2'%((args.num_points,)+tuple(args.pressure_bounds)))
x_evaluate = np.linspace(
    np.log(args.pressure_bounds[0]),
    np.log(args.pressure_bounds[1]),
    args.num_points,
)   
x_evaluate -= 2*np.log(utils.c) ### divide by c^2 to get this into consistent units with gpr-resample, etc

### add stitching stuff
stitch = []
if args.external_process: ### use externally supplied GP as stitching condition
    for path in args.external_process:
        if args.verbose:
            print('loading in external process for stitching from: '+path)
        stitch += gp.hdf5load(path)

if args.stitch: ### set up stitching conditions to a fixed value
    if args.verbose:
        print('enforcing stitching to f=%.3e with pressure-scale=%.3e dyn/cm^2 and index=%.3f at %d points within  [%.3e, %.3e] dyn/cm^2'%\
            ((args.stitch_mean, args.stitch_pressure, args.stitch_index, args.stitch_num_points)+tuple(args.stitch_pressure_bounds)))
    x_stitch = np.linspace(
        np.log(args.stitch_pressure_bounds[0]),
        np.log(args.stitch_pressure_bounds[1]),
        args.stitch_num_points,
    )
    x_stitch -= 2*np.log(utils.c) ### divide by c^2 to get this into consistent units with gpr-resample, etc
    f_stitch, cov_stitch = gp.cov_phi_phi_stitch(x_stitch, args.stitch_mean, args.stitch_pressure, args.stitch_index)
    stitch += [{'x':x_stitch, 'f':f_stitch, 'cov':cov_stitch}]

#------------------------

models = []
for hdf5path in args.hdf5paths:
    if args.verbose:
        print('reading: '+hdf5path)
    models.append(gp.hdf5load(hdf5path))

xlabel=models[0][0]['labels']['xlabel'] ### assume these are the same for all models...
flabel=models[0][0]['labels']['flabel']

### create combinatorically many possible matchings between all models
inds = utils.models2combinations(models)
Ninds = len(inds)
tmp = 'hyperparams %'+str(int(np.floor(np.log10(len(sigma)))))+'d / %d'%len(sigma)+' ; model combination %'+str(int(np.floor(np.log10(Ninds))))+'d / %d'%Ninds
TMP = '    p=%d; s=%.3e; l=%.3f; S=%.3e; m=%.3e'

hdf5path = os.path.join(args.output_dir, 'gpr_gpr%s.hdf5'%args.tag)
if args.verbose:
    print('iterating over %d combinations of input mixture models'%Ninds)
with h5py.File(hdf5path, 'w') as obj:

    counter = 0 ### used when labeling components of the mixture model

    logweights = [] ### holder for all weights before we normalize them
    for hyperparamind, (p, s, l, S, m, w) in enumerate(zip(polydeg, sigma, length_scale, sigma_obs, model_multiplier, weights)):
        s2 = s**2
        l2 = l**2
        S2 = S**2
        m2 = m**2

        logLikes = []

        for ind, indecies in enumerate(inds):
            if args.Verbose:
                print(tmp%(hyperparamind+1, ind+1))
                print(TMP%(p, s, l, S, m))
                print('building big covariance matrix')

            x_obs, f_obs, covs, model_covs, Nstitch = gp.cov_altogether_noise(
                [model[i] for model, i in zip(models, indecies)],
                stitch,
                diagonal_model_covariance=args.diagonal_model_covariance,
            )

            #-------------------------------------------------

            if args.verbose:
                if args.stitch:
                    print('regressing %d values from %d+%d noisy observations+stitching conditions'%(args.num_points, len(x_obs)-Nstitch, Nstitch))
                else:
                    print('regressing %d values from %d noisy observations'%(args.num_points, len(x_obs)))
            mean, cov, logweight = gp.gpr_altogether(
                x_evaluate,
                f_obs, ### used in GPR conditioning and polymodel
                x_obs, ### used in polymodel and adding covariance kernel
                covs, ### only the noise component (includes model multiplier, though)
                model_covs,
                Nstitch,
                degree=p,
                guess_sigma2=s2,
                guess_l2=l2,
                guess_sigma2_obs=S2,
                guess_model_multiplier2=m2,
            )
    
            ### include the prior weights from each mixture model
            for model, j in zip(models, indecies):
                logweight += np.log(model[j]['weight'])

            logLikes.append(logweight)

            ### create hdf5 group for this conditioned process
            if args.Verbose:
                print('    creating hdf5 group')
            gp.create_process_group(
                obj.create_group(str(counter)),
                p,
                s,
                l,
                S,
                x_evaluate,
                mean,
                cov,
                xlabel=xlabel,
                flabel=flabel,
                weight=1., ### will be over-written in short order
                model_multiplier=m,
            )

            counter += 1 ### bump the index for each element of the mixture model  

        ### normalize the weights for each component of mixture model at this set of hyperparams
        if len(logLikes)==1:
            logLikes = np.array([1.0]) ### handle things like infty/infty gracefully here
        else:
            logLikes = np.array(logLikes, dtype=float) / args.temperature
            maximum = np.max(logLikes)
            logLikes = np.exp(logLikes-np.max(logLikes))
            logLikes /= np.sum(logLikes) ### normalize this

        ### add hyperparam weight here!
        logLikes += np.log(w) ### add weight for this set of hyperparams

        ### add these to the overall counter
        logweights += list(logLikes)

    ### update weights for each conditioned model after normalizing all of them
    if args.verbose:
        print('updating weights for all conditioned models so they are normalized')
    logweights = np.array(logweights) / args.temperature
    weights = np.exp(logweights-np.max(logweights))
    weights /= np.sum(weights)

    for counter, w in enumerate(weights): ### update attributes in hdf5 groups
        obj[str(counter)].attrs['weight'] = w

if args.verbose:
    print('process written to: '+hdf5path)

### plot the result
if args.plot:

    ### plot the underlying GPs
    figtup = None
    for model in models:
        for m in model:
            m['x'] = np.exp(m['x']) ### exponentiate the x-variable for plotting purposes
        figtup = plot.overlay_model(
            model,
            color=plot.DEFAULT_COLOR2,
            logx=True,
            logy=False,
            xlabel='$p/c^2$',
            ylabel=args.ylabel,
            fractions=args.fractions,
            residuals=args.residuals,
            ratios=args.ratios,
            grid=args.grid,
            levels=args.level,
            alpha=0.25,
            figtup=figtup,
        )

    ### plot the over-arching GP on top of everything else
    model = gp.hdf5load(hdf5path) ### load in the model we just wrote to disk. Wasteful of I/O, but whatever
    for m in model: ### exponentiate the x-variable for plotting purposes
        m['x'] = np.exp(m['x'])
    figtup = plot.overlay_model(
        model,
        color=plot.DEFAULT_COLOR1,
        logx=True,
        logy=False,
        xlabel='$p/c^2$',
        ylabel=args.ylabel,
        fractions=args.fractions,
        residuals=args.residuals,
        ratios=args.ratios,
        figwidth=args.figwidth,
        figheight=args.figheight,
        grid=args.grid,
        levels=args.level,
        figtup=figtup,
    )

    ### save
    plot.save('gpr-gpr%s'%args.tag, figtup[0], figtypes=args.figtype, directory=args.output_dir, verbose=args.verbose, dpi=args.dpi)
    plot.close(figtup[0])
