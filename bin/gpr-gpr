#!/usr/bin/env python

__description__ = "read in existing processes from disk and generate another process over them."
__author__ = "reed.essick@ligo.org"

#---------------------------------------------------------------------------------------------------

import os

import numpy as np

from argparse import ArgumentParser

### non-standard
from universality import utils
from universality import gaussianprocess as gp
from universality import plot

#-------------------------------------------------

DEFAULT_MIN = 1e30 ### g/cm^3
DEFAULT_MAX = 1e38

DEFAULT_STITCH_MEAN = 6.0 ### chosen by eye...
DEFAULT_STITCH_PRESSURE = 1e10*utils.c2 ### dyn/cm^2
DEFAULT_STITCH_INDEX = 5

#-------------------------------------------------

parser = ArgumentParser(description=__doc__)

### required arguments
parser.add_argument_group('required arguments')
parser.add_argument('pklpaths', nargs='+', type=str)

### verbosity options
vgroup = parser.add_argument_group('verbosity arguments')
parser.add_argument('-v', '--verbose', default=False, action='store_true')

### options for evaluation
ggroup = parser.add_argument_group('Gaussian Process options')
ggroup.add_argument('--pressure-bounds', default=(DEFAULT_MIN, DEFAULT_MAX), nargs=2, type=float,
    help='min max values for evaluation bounds. Specified in the same units used in the supplied pkl. \
DEFAULT=%.3e %.3e'%(DEFAULT_MIN, DEFAULT_MAX))
ggroup.add_argument('-n', '--num-points', default=gp.DEFAULT_NUM, type=int,
    help='evaluate at this number of points. \
DEFAULT=%d'%gp.DEFAULT_NUM)

ggroup.add_argument('--poly-degree', default=gp.DEFAULT_POLY_DEGREE, type=int,
    help='the degree of the polynomial used to model eos before GPR as part of evaluation. \
DEFAULT=%d'%gp.DEFAULT_POLY_DEGREE)

ggroup.add_argument('-s', '--sigma', default=gp.DEFAULT_SIGMA, type=float,
    help='used as a guess for the optimizer. \
DEFAULT=%.3e'%gp.DEFAULT_SIGMA)
ggroup.add_argument('-l', '--length-scale', dest='l', default=gp.DEFAULT_L, type=float,
    help='used as a guess for the optimizer. \
DEFAULT=%.3e'%gp.DEFAULT_L)
ggroup.add_argument('-S', '--sigma-obs', default=gp.DEFAULT_SIGMA, type=float,
    help='used as a guess for the optimizer. \
DEFAULT=%.3e'%gp.DEFAULT_SIGMA)

ggroup.add_argument('-m', '--model-multiplier', default=gp.DEFAULT_MODEL_MULTIPLIER, type=float,
    help='multiplicative factor for theoretical variance. Larger values increase the "theory noise" from the variance between resampled curves. \
Default=%d'%gp.DEFAULT_MODEL_MULTIPLIER)

### options for stitching
sgroup = parser.add_argument_group('stitching options')
sgroup.add_argument('--stitch', default=False, action='store_true')

sgroup.add_argument('--stitch-pressure-bounds', nargs=2, default=None, type=float,
    help='limit the range of stitching points. DEFAULT is to use --pressure-bounds')
sgroup.add_argument('--stitch-num-points', default=None, type=int,
    help='the number of points to use when constructing the stitching conditions. DEFAULT is to use --num-points')

sgroup.add_argument('--stitch-mean', default=DEFAULT_STITCH_MEAN, type=float,
    help='the mean value for stitching points. \
DEFAULT=%.3f'%DEFAULT_STITCH_MEAN)
sgroup.add_argument('--stitch-pressure', default=DEFAULT_STITCH_PRESSURE, type=float,
    help='the abscissa at which we place a roll-off to enforce stitching. \
We construct a white-noise kernel ~ (p/stitch_pressure)**stitch_index. \
DEFAULT=%.3f'%DEFAULT_STITCH_PRESSURE)
sgroup.add_argument('--stitch-index', default=DEFAULT_STITCH_INDEX, type=float,
    help='the power used to roll-off the white-noise kernel and enforce stitching. \
We construct a white-noise kernel ~ (p/stitch_pressure)**stitch_index. \
DEFAULT=%.3f'%DEFAULT_STITCH_INDEX)

### plotting options
pgroup = parser.add_argument_group('plotting options')
pgroup.add_argument('-p', '--plot', default=False, action='store_true')
pgroup.add_argument('--residuals', default=False, action='store_true')

pgroup.add_argument('--ylabel', default='$\phi$', type=str)

pgroup.add_argument('--figwidth', default=plot.DEFAULT_FIGWIDTH, type=float)
pgroup.add_argument('--figheight', default=plot.DEFAULT_FIGHEIGHT, type=float)

pgroup.add_argument('--grid', default=False, action='store_true')

### output options
ogroup = parser.add_argument_group('output options')
ogroup.add_argument('-o', '--output-dir', default='.', type=str)
ogroup.add_argument('-t', '--tag', default='', type=str)
ogroup.add_argument('--figtype', default=[], type=str, action='append')
ogroup.add_argument('--dpi', default=plot.DEFAULT_DPI, type=float)

args = parser.parse_args()

### finish parsing
Npkl = len(args.pklpaths)

if args.stitch_pressure_bounds is None:
    args.stitch_pressure_bounds = args.pressure_bounds
if args.stitch_num_points is None:
    args.stitch_num_points = args.num_points

if not os.path.exists(args.output_dir):
    os.makedirs(args.output_dir)

if args.tag:
    args.tag = "_"+args.tag

if not args.figtype:
    args.figtype = plot.DEFAULT_FIGTYPES

#-------------------------------------------------

labels = None
means = []
covs = []
for pklpath in args.pklpaths:
    if args.verbose:
        print('reading: '+pklpath)
    xlabel, ylabel, x, mean, cov = gp.pklload(pklpath)

    if labels==None:
        labels = xlabel, ylabel
        x_obs = x

    else:
        assert labels==(xlabel,ylabel), 'labels must match in all pkl files!'
        assert np.all(x_obs==x), 'all pkl files must be evaluated at the same x-values!'

    means.append(mean)
    covs.append(cov)

Nx = len(x_obs)

#-------------------------------------------------

if args.verbose:
    print('computing modeling uncertainty at each x-value')

### reshape resampled vectors into 1D arrays
means_1d = np.concatenate(means)
xs_1d = np.concatenate([x_obs for _ in means])
Nobs = len(xs_1d)

### add in "theory model noise" as diagonal components based on variance of means at each pressure
if args.stitch:
    covs_2d = np.zeros((Nobs+args.stitch_num_points, Nobs+args.stitch_num_points), dtype=float) ### include space for the stitching conditions
else:
    covs_2d = np.zeros((Nobs,Nobs), dtype=float)

### iterate through pressure samples and compute theory variance of each
for i in xrange(Nx):
    ### variance for this pressure sample
    var = np.var([m[i] for m in means]) + np.sum([c[i,i] for c in covs])

    # multiply this sample by some factor (hyper-parameter)
    var *= args.model_multiplier

    # fill in big array as needed
    for j in xrange(Npkl):
        ind = j*Nx + i
        covs_2d[ind,ind] += var ### variance goes on the diagonal

### iterate over regression covariances and fill in the matrix
### this includes off-diagonal correlations within each resampling
i=0
for ind in xrange(Npkl):
    j = i+Nx
    covs_2d[i:j,i:j] += covs[ind]
    i = j

#-------------------------------------------------

### perform the "altogether" regression
if args.verbose:
    print('evaluating f at %d points within [%.3e, %.3e] dyn/cm^2'%((args.num_points,)+tuple(args.pressure_bounds)))
x_evaluate = np.linspace(
    np.log(args.pressure_bounds[0]),
    np.log(args.pressure_bounds[1]),
    args.num_points,
)
x_evaluate -= 2*np.log(utils.c) ### divide by c^2 to get this into consistent units with gpr-resample, etc

if args.stitch: ### set up stitching conditions
    if args.verbose:
        print('enforcing stitching to f=%.3e with pressure-scale=%.3e dyn/cm^2 and index=%.3f at %d points within  [%.3e, %.3e] dyn/cm^2'%((args.stitch_mean, args.stitch_pressure, args.stitch_index, args.stitch_num_points)+tuple(args.stitch_pressure_bounds)))
    x_stitch = np.linspace(
        np.log(args.stitch_pressure_bounds[0]),
        np.log(args.stitch_pressure_bounds[1]),
        args.stitch_num_points,
    )
    x_stitch -= 2*np.log(utils.c) ### divide by c^2 to get this into consistent units with gpr-resample, etc
    xs_1d = np.concatenate((xs_1d, x_stitch))
    means_1d = np.concatenate((means_1d, np.ones(args.stitch_num_points, dtype=float)*args.stitch_mean))
    covs_2d[Nobs:,Nobs:] = np.diag(np.exp(x_stitch - np.log(args.stitch_pressure/utils.c2))**args.stitch_index) ### add the stitching white-noise kernel

if args.verbose:
    if args.stitch:
        print('regressing %d values from %d+%d noisy observations+sticthing conditions'%(args.num_points, Nobs, args.stitch_num_points))
    else:
        print('regressing %d values from %d noisy observations'%(args.num_points, Nobs))
mean, cov = gp.gpr_altogether(
    x_evaluate,
    means_1d,
    xs_1d,
    covs_2d,
    degree=args.poly_degree,
    guess_sigma2=args.sigma**2,
    guess_l2=args.l**2,
    guess_sigma2_obs=args.sigma_obs**2,
)

### save the result
pklpath = os.path.join(args.output_dir, 'gpr_gpr%s.pkl'%args.tag)
if args.verbose:
    print('writing process to: '+pklpath)
gp.pkldump(pklpath, labels[0], labels[1], x_evaluate, mean, cov)

### plot the result
if args.plot:
    cr_obs = []
    for m, c in zip(means, covs):
        c = np.diag(c)**0.5
        cr_obs.append( (m-c, m+c) )

    fig = plot.gpr_overlay(
        np.exp(x_evaluate),
        mean,
        (mean-np.diag(cov)**0.5, mean+np.diag(cov)**0.5),
        [np.exp(x_obs)]*Npkl,
        means,
        cr_obs=cr_obs,
        linestyle_obs='-',
        logx=True,
        logy=False,
        xlabel='$p/c^2$',
        ylabel=args.ylabel,
        residuals=args.residuals,
        figwidth=args.figwidth,
        figheight=args.figheight,
        grid=args.grid,
    )
    plot.save('gpr-gpr%s'%args.tag, fig, figtypes=args.figtype, directory=args.output_dir, verbose=args.verbose, dpi=args.dpi)
    plot.plt.close(fig)
