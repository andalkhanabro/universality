#!/usr/bin/env python

__description__ = "read in existing processes from disk and generate another process over them."
__author__ = "reed.essick@ligo.org"

#---------------------------------------------------------------------------------------------------

import os

import numpy as np
import h5py

from argparse import ArgumentParser

### non-standard
from universality import utils
from universality import gaussianprocess as gp
from universality import plot

#-------------------------------------------------

DEFAULT_MIN = 1e30 ### g/cm^3
DEFAULT_MAX = 1e38

DEFAULT_STITCH_MEAN = 6.0 ### chosen by eye...
DEFAULT_STITCH_PRESSURE = 1e10*utils.c2 ### dyn/cm^2
DEFAULT_STITCH_INDEX = 5

#-------------------------------------------------

parser = ArgumentParser(description=__doc__)

### required arguments
parser.add_argument_group('required arguments')
parser.add_argument('hdf5paths', nargs='+', type=str)

### verbosity options
vgroup = parser.add_argument_group('verbosity arguments')
vgroup.add_argument('-v', '--verbose', default=False, action='store_true')
vgroup.add_argument('-V', '--Verbose', default=False, action='store_true')

### options for evaluation
ggroup = parser.add_argument_group('Gaussian Process options')
ggroup.add_argument('--pressure-bounds', default=(DEFAULT_MIN, DEFAULT_MAX), nargs=2, type=float,
    help='min max values for evaluation bounds. Specified in the same units used in the supplied pkl. \
DEFAULT=%.3e %.3e'%(DEFAULT_MIN, DEFAULT_MAX))
ggroup.add_argument('-n', '--num-points', default=gp.DEFAULT_NUM, type=int,
    help='evaluate at this number of points. \
DEFAULT=%d'%gp.DEFAULT_NUM)

ggroup.add_argument('--hyperparampath', default=None, type=str,
    help='a path to a csv with the following columns: "polydeg", "sigma", "l", "sigma_obs", "loglike" such as is produced by investigate-hyperparams. \
If supplied, we ignore values of --sigma, --length-scale, --sigma-obs and build a mixture-model hdf5 file based on the weights contained in the csv.')
ggroup.add_argument('--max-num-models', default=None, type=int,
    help='if supplied, only include up to this many models in the mixture, selecting those with the largest weights first')

ggroup.add_argument('--poly-degree', default=gp.DEFAULT_POLY_DEGREE, type=int,
    help='the degree of the polynomial used to model eos before GPR as part of evaluation. \
DEFAULT=%d'%gp.DEFAULT_POLY_DEGREE)

ggroup.add_argument('-s', '--sigma', default=gp.DEFAULT_SIGMA, type=float,
    help='used as a guess for the optimizer. \
DEFAULT=%.3e'%gp.DEFAULT_SIGMA)
ggroup.add_argument('-l', '--length-scale', dest='l', default=gp.DEFAULT_L, type=float,
    help='used as a guess for the optimizer. \
DEFAULT=%.3e'%gp.DEFAULT_L)
ggroup.add_argument('-S', '--sigma-obs', default=gp.DEFAULT_SIGMA, type=float,
    help='used as a guess for the optimizer. \
DEFAULT=%.3e'%gp.DEFAULT_SIGMA)

ggroup.add_argument('-m', '--model-multiplier', default=gp.DEFAULT_MODEL_MULTIPLIER, type=float,
    help='multiplicative factor for theoretical variance. Larger values increase the "theory noise" from the variance between resampled curves. \
Default=%d'%gp.DEFAULT_MODEL_MULTIPLIER)

### options for stitching
sgroup = parser.add_argument_group('stitching options')
sgroup.add_argument('--stitch', default=False, action='store_true')

sgroup.add_argument('--stitch-pressure-bounds', nargs=2, default=None, type=float,
    help='limit the range of stitching points. DEFAULT is to use --pressure-bounds')
sgroup.add_argument('--stitch-num-points', default=None, type=int,
    help='the number of points to use when constructing the stitching conditions. DEFAULT is to use --num-points')

sgroup.add_argument('--stitch-mean', default=DEFAULT_STITCH_MEAN, type=float,
    help='the mean value for stitching points. \
DEFAULT=%.3f'%DEFAULT_STITCH_MEAN)
sgroup.add_argument('--stitch-pressure', default=DEFAULT_STITCH_PRESSURE, type=float,
    help='the abscissa at which we place a roll-off to enforce stitching. \
We construct a white-noise kernel ~ (p/stitch_pressure)**stitch_index. \
DEFAULT=%.3f'%DEFAULT_STITCH_PRESSURE)
sgroup.add_argument('--stitch-index', default=DEFAULT_STITCH_INDEX, type=float,
    help='the power used to roll-off the white-noise kernel and enforce stitching. \
We construct a white-noise kernel ~ (p/stitch_pressure)**stitch_index. \
DEFAULT=%.3f'%DEFAULT_STITCH_INDEX)

### plotting options
pgroup = parser.add_argument_group('plotting options')
pgroup.add_argument('-p', '--plot', default=False, action='store_true')
pgroup.add_argument('--fractions', default=False, action='store_true')
pgroup.add_argument('--residuals', default=False, action='store_true')
pgroup.add_argument('--ratios', default=False, action='store_true')

pgroup.add_argument('--ylabel', default='$\phi$', type=str)

pgroup.add_argument('--figwidth', default=plot.DEFAULT_FIGWIDTH, type=float)
pgroup.add_argument('--figheight', default=plot.DEFAULT_FIGHEIGHT, type=float)

pgroup.add_argument('--grid', default=False, action='store_true')

### output options
ogroup = parser.add_argument_group('output options')
ogroup.add_argument('-o', '--output-dir', default='.', type=str)
ogroup.add_argument('-t', '--tag', default='', type=str)
ogroup.add_argument('--figtype', default=[], type=str, action='append')
ogroup.add_argument('--dpi', default=plot.DEFAULT_DPI, type=float)

args = parser.parse_args()

### finish parsing
Nhdf5 = len(args.hdf5paths)

if args.stitch_pressure_bounds is None:
    args.stitch_pressure_bounds = args.pressure_bounds
if args.stitch_num_points is None:
    args.stitch_num_points = args.num_points

if not os.path.exists(args.output_dir):
    os.makedirs(args.output_dir)

if args.tag:
    args.tag = "_"+args.tag

if not args.figtype:
    args.figtype = plot.DEFAULT_FIGTYPES

args.verbose |= args.Verbose

#-------------------------------------------------

# figure out the hyperparameters we'll use
if args.hyperparampath:
    polydeg, sigma, length_scale, sigma_obs, model_multiplier, weights = utils.load(args.hyperparampath, ['poly_degree', 'sigma', 'l', 'sigma_obs', 'multiplier', 'logLike'])[0].transpose()
    polydeg = polydeg.astype(int)

    if args.max_num_models is not None: ### truncate the number of models considered
        truth = weights.argsort()[::-1][:args.max_num_models]
        polydeg = polydeg[truth]
        sigma = sigma[truth]
        length_scale = length_scale[truth]
        sigma_obs = sigma_obs[truth]
        weights = weights[truth]

    weights = np.exp(weights-np.max(weights))
    weights /= np.sum(weights)

    truth = weights > 0 ### throw away anything that won't ever matter
    polydeg = polydeg[truth]
    sigma = sigma[truth]
    length_scale = length_scale[truth]
    sigma_obs = sigma_obs[truth]
    weights = weights[truth]

else:
    polydeg = [args.poly_degree]
    sigma = [args.sigma]
    length_scale = [args.l]
    sigma_obs = [args.sigma_obs]
    model_multiplier = [args.model_multiplier]
    weights = [1.]

#-------------------------------------------------

models = []
for hdf5path in args.hdf5paths:
    if args.verbose:
        print('reading: '+hdf5path)
    models.append(gp.hdf5load(hdf5path))

### create combinatorically many possible matchings between all models
inds = zip(*[_.flatten() for _ in np.meshgrid(*[range(len(model)) for model in models])])
Ninds = len(inds)

#-------------------------------------------------

### set up the evaluation points, which are use repeatedly within the following loop
if args.verbose:
    print('evaluating f at %d points within [%.3e, %.3e] dyn/cm^2'%((args.num_points,)+tuple(args.pressure_bounds)))
x_evaluate = np.linspace(
    np.log(args.pressure_bounds[0]),
    np.log(args.pressure_bounds[1]),
    args.num_points,
)   
x_evaluate -= 2*np.log(utils.c) ### divide by c^2 to get this into consistent units with gpr-resample, etc

### add stitching stuff
if args.stitch: ### set up stitching conditions
    if args.verbose:
        print('enforcing stitching to f=%.3e with pressure-scale=%.3e dyn/cm^2 and index=%.3f at %d points within  [%.3e, %.3e] dyn/cm^2'%\
            ((args.stitch_mean, args.stitch_pressure, args.stitch_index, args.stitch_num_points)+tuple(args.stitch_pressure_bounds)))
    x_stitch = np.linspace(
        np.log(args.stitch_pressure_bounds[0]),
        np.log(args.stitch_pressure_bounds[1]),
        args.stitch_num_points,
    )
    x_stitch -= 2*np.log(utils.c) ### divide by c^2 to get this into consistent units with gpr-resample, etc
    f_stitch = np.ones(args.stitch_num_points, dtype=float)*args.stitch_mean
    cov_stitch = np.diag(np.exp(x_stitch - np.log(args.stitch_pressure/utils.c2))**args.stitch_index) ### the stitching white-noise kernel

#------------------------

hdf5path = os.path.join(args.output_dir, 'gpr_gpr%s.hdf5'%args.tag)
if args.verbose:
    print('iterating over %d combinations of input mixture models\nwriting process to: %s'%(Ninds, hdf5path))
with h5py.File(hdf5path, 'w') as obj:

    ### place-holder for when we iterate over many possible hyperparam values
    ind = 0 ### counter for each element of the mixture model

    logweights = [] ### holder for all weights before we normalize them
    for p, s, l, S, m, w in zip(polydeg, sigma, length_scale, sigma_obs, model_multiplier, weights):
        s2 = s**2
        l2 = l**2
        S2 = S**2

        logLikes = []
        tmp = '%'+str(int(np.floor(np.log10(Ninds))))+'d / %d'%Ninds
        for indecies in inds:
            if args.Verbose:
                print(tmp%(ind+1))

            ### iterate over the models included here and extract the relevant parameters for this combination of models
            if args.Verbose:
                print('    extracting data from each model included in this combination')
            x_obs = []
            f_obs = []
            xlabel = models[0][0]['labels']['xlabel']
            flabel = models[0][0]['labels']['flabel']
            for model, j in zip(models, indecies):
                assert xlabel==model[j]['labels']['xlabel'], 'xlabel must match for all models!'
                assert flabel==model[j]['labels']['flabel'], 'flabel must match for all models!'
                x_obs.append(model[j]['x'])
                f_obs.append(model[j]['f'])
            x_obs = np.concatenate(x_obs)
            f_obs = np.concatenate(f_obs)

            ### compute the big uncertainty estimate between all the models
            Nobs = len(x_obs)

            ### add in "theory model noise" as diagonal components based on variance of means at each pressure
            if args.Verbose:
                print('    assembling measurement uncertainty covariance')
            if args.stitch:
                covs = np.zeros((Nobs+args.stitch_num_points, Nobs+args.stitch_num_points), dtype=float) ### include space for the stitching conditions

                x_obs = np.concatenate((x_obs, x_stitch)) ### add in stitching data
                f_obs = np.concatenate((f_obs, f_stitch))
                covs[Nobs:,Nobs:] = cov_stitch

            else:
                covs = np.zeros((Nobs,Nobs), dtype=float)

            ### add block-diagonal components
            if args.Verbose:
                print('    adding block-diagonal contributions')
            start = 0
            for model, j in zip(models, indecies):
                stop = start+len(model[j]['x']) ### the number of points in this model
                covs[start:stop,start:stop] = model[j]['cov'] ### fill in block-diagonal component
                start = stop

            ### iterate through pressure samples and compute theory variance of each
            if args.verbose:
                print('    adding modeling uncertainty at each x-value')

            ### NOTE: the following iteration may not be the most efficient thing in the world, but it should get the job done...
            truth = np.empty(covs.shape[0], dtype=bool) ### used to index coveriance matrix when filling things in
            for x in set(x_obs): # iterate over all included x-points

                truth[:] = False ### re-set this
    
                sample = [] # set up holders for the values from each model that covers this pressure point
                sample_covs = []

                ### iterate over each model, checking to see whether it has this x-value
                start = 0
                for model, j in zip(models, indecies):
                    n = len(model[j]['x'])
                    i = np.arange(n)[x==model[j]['x']]
                    if len(i): ### there's a match
                        i = i[0]
                        truth[start+i] = True ### we update this index in the big covariance matrix
                        sample.append(model[j]['f'][i]) ### record the mean
                        sample_covs.append(model[j]['cov'][i,i]) ### and the covariance
                    start += n ### bump this

                ### add the sample variance for this pressure mulitplied by some factor (hyper-parameter)
                covs[truth,truth] += (np.var(sample) + np.sum(sample_covs))*m

            #-------------------------------------------------

            if args.verbose:
                if args.stitch:
                    print('regressing %d values from %d+%d noisy observations+sticthing conditions'%(args.num_points, Nobs, args.stitch_num_points))
                else:
                    print('regressing %d values from %d noisy observations'%(args.num_points, Nobs))
            mean, cov, logweight = gp.gpr_altogether(
                x_evaluate,
                f_obs,
                x_obs,
                covs,
                degree=p,
                guess_sigma2=s2,
                guess_l2=l2,
                guess_sigma2_obs=S2,
            )
    
            ### include the prior weights from each mixture model
            for model, j in zip(models, indecies):
                logweight += np.log(model[j]['weight'])

            logLikes.append(logweight)

            ### create hdf5 group for this conditioned process
            if args.Verbose:
                print('    creating hdf5 group')
            gp.create_process_group(
                obj.create_group(str(ind)),
                p,
                s,
                l,
                S,
                x_evaluate,
                mean,
                cov,
                xlabel=xlabel,
                flabel=flabel,
                weight=1., ### will be over-written in short order
                model_multiplier=m,
            )

            ind += 1 ### bump the index for each element of the mixture model  

        ### normalize the weights for each component of mixture model at this set of hyperparams
        if len(logLikes)==1:
            logLikes = np.array([1]) ### handle things like infty/infty gracefully here
        else:
            logLikes = np.array(logLikes)
            maximum = np.max(logLikes)
            logLikes = np.exp(logLikes-np.max(logLikes))
            logLikes /= np.sum(logLikes) ### normalize this

        ### add hyperparam weight here!
        logLikes += np.log(w) ### add weight for this set of hyperparams

        ### add these to the overall counter
        logweights += list(logLikes)

    ### update weights for each conditioned model after normalizing all of them
    if args.verbose:
        print('updating weights for all conditioned models so they are normalized')
    logweights = np.array(logweights)
    weights = np.exp(logweights-np.max(logweights))
    weights /= np.sum(weights)

    for ind, w in enumerate(weights): ### update attributes in hdf5 groups
        obj[str(ind)].attrs['weight'] = w

### plot the result
if args.plot:

    ### plot the over-arching GP
    figtup = plot.overlay_model(
        gp.hdf5load(hdf5path), ### load in the model we just wrote to disk. Wasteful of I/O, but whatever
        color=plot.DEFAULT_COLOR1,
        logx=True,
        logy=False,
        xlabel='$p/c^2$',
        ylabel=args.ylabel,
        fractions=args.fractions,
        residuals=args.residuals,
        ratios=args.ratios,
        figwidth=args.figwidth,
        figheight=args.figheight,
        grid=args.grid,
    )

    ### plot the underlying GPs
    for model in models:
        plot.overlay_model(
            model,
            color=plot.DEFAULT_COLOR2
            logx=True,
            logy=False,
            xlabel='$p/c^2$',
            ylabel=args.ylabel,
            fractions=args.fractions,
            residuals=args.residuals,
            ratios=args.ratios,
            grid=args.grid,
            figtup=figtup,
        )

    ### save
    plot.save('gpr-gpr%s'%args.tag, figtup[0], figtypes=args.figtype, directory=args.output_dir, verbose=args.verbose, dpi=args.dpi)
    plot.close(fig)
