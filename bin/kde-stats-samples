#!/usr/bin/env python

"""generate statistics based on the KDE over these samples
"""
__author__ = "Reed Essick (reed.essick@gmail.com)"

#-------------------------------------------------

import numpy as np

from argparse import ArgumentParser

### non-standard libraries
from universality.utils import (utils, io)
from universality import kde
from universality import stats
from universality import plot

#-------------------------------------------------

parser = ArgumentParser(description=__doc__)

parser.add_argument('samples', type=str)
parser.add_argument('columns', nargs='+', type=str)

vgroup = parser.add_argument_group('verbosity options')
vgroup.add_argument('-v', '--verbose', default=False, action='store_true')

sgroup = parser.add_argument_group('samples-specific argument')
sgroup.add_argument('--max-num-samples', default=io.DEFAULT_MAX_NUM_SAMPLES, type=int)
sgroup.add_argument('--weight-column', default=[], type=str, action='append',
    help='if provided, thie numerical values from this column will be used as weights in the KDE')
sgroup.add_argument('--weight-column-is-log', default=[], type=str, action='append',
    help='if supplied, interpret the values in weight_column as log(weight), meaning we exponentiate them before using them in the KDE')
sgroup.add_argument('--invert-weight-column', default=[], type=str, action='append',
    help='the column for which this is true')

cgroup = parser.add_argument_group('column-specific arguments')
cgroup.add_argument('--logcolumn', default=[], type=str, action='append',
    help='convert the values read in for this column to natural log. \
Can be repeated to specify multiple columns. \
DEFAULT=[]')

cgroup.add_argument('--column-range', nargs=3, default=[], action='append', type=str,
    help='specify the ranges used in corner.corner (eg.: "--range column minimum maximum"). \
Can specify ranges for multiple columns by repeating this option. \
DEFAULT will use the minimum and maximum observed sample points.')
cgroup.add_argument('--column-bandwidth', nargs=2, default=[], type=str, action='append',
    help='the bandwidths used for each column specified. We assume diagonal covariance matricies in the Gaussian kernel. \
If you do not specify a bandwidth for a column, the default value (%.3f) will be used.'%kde.DEFAULT_BANDWIDTH)

cgroup.add_argument('--column-multiplier', nargs=2, default=[], type=str, action='append',
    help='multiply the column by this number before computing statistics. This is applied when data is first read in \
(but after taking the natural log if --logcolumn is specified), meaning bandwidths and ranges should be specified in \
terms of the column times the multiplier. If no multiplier is specified for a column, that column is left unchanged.')

wgroup = parser.add_argument_group('workflow options')
wgroup.add_argument('--num-points', default=plot.DEFAULT_NUM_POINTS, type=int,
    help='DEFAULT=%d'%plot.DEFAULT_NUM_POINTS)
wgroup.add_argument('--reflect', default=False, action='store_true',
    help='reflect the points about their boundaries within the KDE')
wgroup.add_argument('--prune', default=False, action='store_true',
    help='throw away samples that fall outside ranges')

ogroup = parser.add_argument_group('options about what to compute')
ogroup.add_argument('--one-dim-confidence-region-samples', default=[], type=float, action='append',
    help='compute the boundaries of the 1D confidence regions using only weighted samples (no KDE) for each column corresponding to this confidence. Can be repeated.')
ogroup.add_argument('--one-dim-confidence-region', default=[], type=float, action='append',
    help='compute the boundaries of the 1D confidence regions for each column corresponding to this confidence. Can be repeated.')
ogroup.add_argument('--one-dim-confidence-region-format4tex', default=False, action='store_true')
ogroup.add_argument('--one-dim-confidence-region-scientific-notation', type=str, default=[], action='append',
    help="print this column's result in scientific notation")

ogroup.add_argument('--confidence-region-size', default=[], type=float, action='append',
    help='compute the confidence region volume for this confidence level [0.0, 1.0]. Can be repeated.')

ogroup.add_argument('--entropy', default=False, action='store_true')
ogroup.add_argument('--information', default=False, action='store_true')

ogroup.add_argument('--argmax', default=False, action='store_true')
ogroup.add_argument('--dlogL', default=[], type=str, action='append',
    help='comma separated list of the parameter values used within dlogL. Can be repeated.')
ogroup.add_argument('--dtheta', default=[], type=str, action='append',
    help='comma separated list of the parameter values used within dtheta. Can be repeated.')
ogroup.add_argument('--confidence-level', default=[], type=str, action='append',
    help='comma separated list of the parameter values used within dtheta. Can be repeated.')

args = parser.parse_args()
Ncol = len(args.columns)

# finish parsing
rangesdict = dict((column,(float(_min), float(_max))) for column, _min, _max in args.column_range)

args.dlogL = [[float(_) for _ in v.split(',')] for v in args.dlogL]
args.dtheta = [[float(_) for _ in v.split(',')] for v in args.dtheta]
args.confidence_level = [[float(_) for _ in v.split(',')] for v in args.confidence_level]

bandwidthdict = dict((col, float(val)) for col, val in args.column_bandwidth)
variances = np.array([bandwidthdict.get(col, kde.DEFAULT_BANDWIDTH) for col in args.columns])**2
multiplierdict = dict((col, float(val)) for col, val in args.column_multiplier)

#-------------------------------------------------

### read in data from csv
if args.verbose:
    print('reading samples from: '+args.samples)
data, columns = io.load(args.samples, args.columns, logcolumns=args.logcolumn, max_num_samples=args.max_num_samples)

### multiplying
for i, column in enumerate(columns):
    if column in multiplierdict:
        data[:,i] *= multiplierdict[column]

ranges = []
for i, col in enumerate(columns):
    if rangesdict.has_key(col):
        ranges.append(rangesdict[col])
    else:
        ranges.append((np.min(data[:,i]), np.max(data[:,i])))

if args.weight_column:
    if args.verbose:
        print('reading in non-trivial weights from: '+args.samples)
    weights = io.load_weights(
        args.samples,
        args.weight_column,
        logweightcolumns=args.weight_column_is_log,
        invweightcolumns=args.invert_weight_column,
        max_num_samples=args.max_num_samples,
    )

else:
    N = len(data)
    weights = np.ones(N, dtype='float')/N

#------------------------

vects = [np.linspace(m, M, args.num_points) for m, M in ranges]

if args.prune:
    data, weights = utils.prune(data, ranges, weights=weights)

#------------------------

if args.one_dim_confidence_region_samples:
    args.one_dim_confidence_region_samples.sort()
    for i, (col, argmax) in enumerate(zip(columns, data[weights.argmax()])):
        mean = stats.samples2mean(data[:,i], weights=weights)
        median = stats.samples2median(data[:,i], weights=weights)

        scinote = col in args.one_dim_confidence_region_scientific_notation

        tup = (col, multiplierdict.get(col, 1), argmax, mean, median, np.min(data[:,i]), np.max(data[:,i]))
        if scinote:
            print('''%s x %.3e
  maxL   = %.6e
  mean   = %.6e
  median = %.6e
  min    = %.6e
  max    = %.6e'''%tup)
        else:
            print('''%s x %.3e
  maxL   = %.6f
  mean   = %.6f
  median = %.6f
  min    = %.6f
  max    = %.6f'''%tup)

        for level, (low, high) in zip(args.one_dim_confidence_region_samples, stats.samples2crbounds(data[:,i], args.one_dim_confidence_region_samples, weights=weights)):
            if args.one_dim_confidence_region_format4tex:
                if scinote:
                    print('    @%.3f maxL region   : %.6e^{+%.6e}_{-%.6e}'%(level, argmax, high-argmax, argmax-low))
                    print('    @%.3f mean region   : %.6e^{+%.6e}_{-%.6e}'%(level, mean, high-mean, mean-low))
                    print('    @%.3f median region : %.6e^{+%.6e}_{-%.6e}'%(level, median, high-median, median-low))
                else:
                    print('    @%.3f maxL region   : %.6f^{+%.6f}_{-%.6f}'%(level, argmax, high-argmax, argmax-low))
                    print('    @%.3f mean region   : %.6f^{+%.6f}_{-%.6f}'%(level, mean, high-mean, mean-low))
                    print('    @%.3f median region : %.6f^{+%.6f}_{-%.6f}'%(level, median, high-median, median-low))
            else:
                if scienote:
                    print('    @%.3f region: [%.3e, %.3e]'%(level, low, high))
                else:
                    print('    @%.3f region: [%.6f, %.6f]'%(level, low, high))

#------------------------

if args.reflect:
    data, weights = utils.reflect(data, ranges, weights=weights)

#------------------------

if args.one_dim_confidence_region:
    if args.verbose:
        print('computing 1D statistics')

    args.one_dim_confidence_region.sort()
    for i, col in enumerate(columns):
        _logkde = kde.logkde(vects[i], data[:,i], variances[i], weights=weights)
        argmax = vects[i][_logkde.argmax()]
        print('%s x %.3e argmax : %.3e'%(col, multiplierdict.get(col, 1), argmax))
        for level, (low, high) in zip(args.one_dim_confidence_region, stats.logkde2crbounds(vects[i], _logkde, args.one_dim_confidence_region)):
            if args.one_dim_confidence_region_format4tex:
                print('    @%.3f kde region: %.6f^{+%.6f}_{-%.6f}'%(level, argmax, high-argmax, argmax-low))
            else:
                print('    @%.3f kde region: [%.3e, %.3e]'%(level, low, high))

#------------------------

if args.confidence_region_size or args.entropy or args.information or args.argmax or args.dlogL or args.dtheta or args.confidence_level:

    if args.verbose:
        print('computing %dD kde'%Ncol)

    logkde = kde.logkde(
        kde.vects2flatgrid(*vects),
        data,
        variances,
        weights=weights,
    )

    if args.verbose:
        print('computing statistics')

    if args.confidence_region_size:
        args.confidence_region_size.sort()
        for level, vol in zip(args.confidence_region_size, stats.logkde2crsize(vects, logkde, args.confidence_region_size)):
            print('Volume(CR=%.3f) = %.6e'%(level, vol))
    
    if args.entropy:
        print('H = %.6e'%stats.logkde2entropy(vects, logkde))

    if args.information:
        print('I = %.6e'%stats.logkde2information(vects, logkde))

    if args.argmax:
        print('argmax = %s'%(stats.logkde2argmax(vects, logkde)))

    if args.dlogL:
        maxlogL = np.max(logkde)
        for point in args.dlogL:
            print('dlogL(%s) = %.6e'%(point, kde.logkde(np.array([point]), data, variances, weights=weights) - maxlogL))

    if args.dtheta:
        for point in args.dtheta:
            print('dtheta(%s) = %.6e'%(point, stats.dtheta(point, vects, logkde)))

    if args.confidence_level:
        for point in args.confidence_level:
            thr = kde.logkde(np.array([point]), data, variances, weights=weights)
            print('confidence level(%s) = %.6e'%(point, 1.*np.sum(logkde>=thr)/len(logkde)))
